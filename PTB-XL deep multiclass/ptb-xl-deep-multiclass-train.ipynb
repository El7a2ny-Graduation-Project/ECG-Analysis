{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1905968,"sourceType":"datasetVersion","datasetId":1136210},{"sourceId":11705526,"sourceType":"datasetVersion","datasetId":7347383}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wfdb\nimport pandas as pd\nimport numpy as np\nimport wfdb\nimport ast\nimport os\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n# Function to load raw data\ndef load_raw_data(df, path):\n    \"\"\"\n    Load raw ECG data at a fixed sampling frequency of 500 Hz.\n\n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        DataFrame containing ECG metadata, including file paths.\n    path : str\n        Base path to the PTB-XL dataset.\n\n    Returns:\n    --------\n    numpy.ndarray\n        Array of raw ECG signals.\n    \"\"\"\n    data = [wfdb.rdsamp(path + f) for f in df.filename_hr]\n    return np.array([signal for signal, meta in data])\n\n# Base path for the PTB-XL dataset\npath = '/kaggle/input/ptb-xl-dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/'\n\n# Fixed classification type and lead type\nclassification_type = \"superclasses\"  # {\"binary\", \"superclasses\", \"subclasses\"}\nlead_types = {\n    \"lead-I\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n    \"bipolar-limb\": [3, 4, 5, 6, 7, 8, 9, 10, 11],\n    \"unipolar-limb\": [0, 1, 2, 6, 7, 8, 9, 10, 11],\n    \"limb-leads\": [6, 7, 8, 9, 10, 11],\n    \"precordial-leads\": [0, 1, 2, 3, 4, 5],\n    \"all-lead\": [],\n}\nlead_name = \"lead-I\"\n\n# Load and convert annotation data\nY = pd.read_csv(path + 'ptbxl_database.csv', index_col='ecg_id')\nY.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\n\n# Load raw ECG data\nX = load_raw_data(Y, path)\n\n# Load scp_statements.csv for diagnostic aggregation\nagg_df = pd.read_csv(path + 'scp_statements.csv', index_col=0)\nagg_df = agg_df[agg_df.diagnostic == 1]\n\n# Aggregation functions for superclasses and subclasses\ndef aggregate_superclass_diagnostic(y_dic):\n    return list(set(agg_df.loc[key].diagnostic_class for key in y_dic.keys() if key in agg_df.index))\n\ndef aggregate_subclass_diagnostic(y_dic):\n    return list(set(agg_df.loc[key].diagnostic_subclass for key in y_dic.keys() if key in agg_df.index))\n\n# Apply diagnostic aggregation\nif classification_type == \"superclasses\":\n    Y['diagnostic_superclass'] = Y.scp_codes.apply(aggregate_superclass_diagnostic)\nelse:\n    Y['diagnostic_superclass'] = Y.scp_codes.apply(aggregate_subclass_diagnostic)\n\n# Split data into train and test sets\ntest_fold = 10\nX_train = X[np.where(Y.strat_fold != test_fold)]\ny_train = Y[Y.strat_fold != test_fold].diagnostic_superclass.tolist()\n\nX_test = X[np.where(Y.strat_fold == test_fold)]\ny_test = Y[Y.strat_fold == test_fold].diagnostic_superclass.tolist()\n\n# Binary classification (if applicable)\nif classification_type == \"binary\":\n    for idx, labels in enumerate(y_train):\n        y_train[idx] = 1 if 'NORM' in labels else 0\n    for idx, labels in enumerate(y_test):\n        y_test[idx] = 1 if 'NORM' in labels else 0\n\n# Reshape data for selected lead type\ndef preprocess_lead_data(data, lead_indices):\n    \"\"\"\n    Remove unwanted leads and reshape the data.\n\n    Parameters:\n    -----------\n    data : numpy.ndarray\n        Raw ECG signal data.\n    lead_indices : list\n        Indices of leads to remove.\n\n    Returns:\n    --------\n    numpy.ndarray\n        Preprocessed ECG data.\n    \"\"\"\n    processed_data = []\n    for ecg in data:\n        processed_data.append(np.delete(ecg, lead_indices, axis=1))\n    return np.array(processed_data)\n\nlist_train = preprocess_lead_data(X_train, lead_types[lead_name])\nlist_test = preprocess_lead_data(X_test, lead_types[lead_name])\n\n# Save processed data\nnp.save('/kaggle/working/x_train.npy', np.array(list_train))\nnp.save('/kaggle/working/x_test.npy', np.array(list_test))\n\n# Convert labels to binary matrices\nmlb = MultiLabelBinarizer()\ny_train = mlb.fit_transform(y_train)\ny_test = mlb.transform(y_test)\n\n# Save labels\nnp.save('/kaggle/working/y_train.npy', y_train)\nnp.save('/kaggle/working/y_test.npy', y_test)\n\nprint(\"Preprocessing complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:39:19.312274Z","iopub.execute_input":"2025-05-10T14:39:19.312887Z","iopub.status.idle":"2025-05-10T14:42:15.574736Z","shell.execute_reply.started":"2025-05-10T14:39:19.312861Z","shell.execute_reply":"2025-05-10T14:42:15.573965Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wfdb in /usr/local/lib/python3.11/dist-packages (4.3.0)\nRequirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.11/dist-packages (from wfdb) (3.11.16)\nRequirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2025.3.2)\nRequirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from wfdb) (3.7.5)\nRequirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from wfdb) (1.26.4)\nRequirement already satisfied: pandas>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2.2.3)\nRequirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2.32.3)\nRequirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (1.15.2)\nRequirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (0.13.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.19.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (24.2)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->wfdb) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->wfdb) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->wfdb) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->wfdb) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->wfdb) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->wfdb) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (2025.1.31)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.10.0->wfdb) (1.17.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.22)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.4->wfdb) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.4->wfdb) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.4->wfdb) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.4->wfdb) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.4->wfdb) (2024.2.0)\nPreprocessing complete.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\n\"\"\"###Import Libraries\"\"\"\nfrom tensorflow.keras.layers import (Conv2D,Conv1D, Add,Activation,\n                                     Dropout,Dense,Flatten,Input,BatchNormalization,\n                                      ReLU,MaxPooling2D,Concatenate,GlobalAveragePooling2D,MaxPooling1D,GlobalAveragePooling1D\n                                     )\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import optimizers, losses, metrics, regularizers, callbacks\nfrom tensorflow.keras.models import Model\nimport tensorflow as tf\nimport numpy as np\nimport os\n\n\"\"\"#### Import Data\"\"\"\n\npath = '/kaggle/working/'\n\ncalssificatin_type = {\"binary\":1,\"superclasses\":5,\"subclasses\":23}\nclassification_name=\"superclasses\"\nno_of_classes=calssificatin_type[classification_name]\n\nlead_type={\"lead-I\":1, \"bipolar-limb\":3 , \"unipolar-limb\":3, \"limb-leads\":6 , \"precordial-leads\":6,\"all-lead\":12}\nlead_name= \"lead-I\"\nno_of_leads=lead_type[lead_name]\n\nx_train = np.load(path + 'x_train.npy', allow_pickle=True)\nx_test  = np.load(path + 'x_test.npy', allow_pickle=True)\ny_train = np.load(path + 'y_train.npy', allow_pickle=True)\ny_test  = np.load(path + 'y_test.npy', allow_pickle=True)\n\nprint(\"Original shapes:\")\nprint(\"x_train:\", x_train.shape)\nprint(\"x_test:\", x_test.shape)\n\nx_train = x_train.transpose(0, 2, 1).reshape(-1, 5000, 1)\nx_test = x_test.transpose(0, 2, 1).reshape(-1, 5000, 1)\nprint(\"After transpose:\")\nprint(\"x_train:\", x_train.shape)\nprint(\"x_test:\", x_test.shape)\n\n# Use actual dimensions from the arrays, not hardcoded values\nactual_train_samples = x_train.shape[0]\nactual_test_samples = x_test.shape[0]\n\nprint(f\"Using {actual_train_samples} training samples and {actual_test_samples} test samples\")\n\n\nprint(\"Final shapes:\")\nprint(\"x_train:\", x_train.shape)\nprint(\"x_test:\", x_test.shape)\n\nprint(\"x_train :\", x_train.shape)\nprint(\"y_train :\", y_train.shape)\nprint(\"x_test  :\", x_test.shape)\nprint(\"y_test  :\", y_test.shape)\nprint('Data loaded')\n\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n\n\n\"\"\"#### Model\"\"\"\n\n\n\n# Define the input shape for 5000 time points and 1 lead\ninput = Input(shape=(5000, no_of_leads))  # Note: Transpose data to match this shape (5000, 1)\n\n# First convolutional block\nconv1 = Conv1D(filters=32, kernel_size=15, strides=1, padding='same')(input)\nbatch1 = BatchNormalization()(conv1)\nrelu1 = ReLU()(batch1)\n\n# Second convolutional block\nconv2 = Conv1D(filters=64, kernel_size=15, strides=2, padding='same')(relu1)\nbatch2 = BatchNormalization()(conv2)\nrelu2 = ReLU()(batch2)\ndrop2 = Dropout(rate=0.1)(relu2)\n\n# Shortcut connection\nmax1 = MaxPooling1D(pool_size=5, strides=2, padding='same')(relu1)\nconv_ = Conv1D(64, kernel_size=1, strides=1, padding='same')(max1)\nconc1 = Add()([conv2, conv_])\n\n# Third convolutional block\nbatch3 = BatchNormalization()(conc1)\nrelu3 = ReLU()(batch3)\ndrop3 = Dropout(rate=0.1)(relu3)\nconv3 = Conv1D(filters=128, kernel_size=15, strides=2, padding='same')(drop3)\nbatch3 = BatchNormalization()(conv3)\nrelu3 = ReLU()(batch3)\ndrop3 = Dropout(rate=0.1)(relu3)\n\n# Shortcut connection\nmax2 = MaxPooling1D(pool_size=5, strides=2, padding='same')(conc1)\nconv_ = Conv1D(128, kernel_size=1, strides=1, padding='same')(max2)\nconc2 = Add()([conv3, conv_])\n\n# Fourth convolutional block\nbatch4 = BatchNormalization()(conc2)\nrelu4 = ReLU()(batch4)\ndrop4 = Dropout(rate=0.1)(relu4)\nconv4 = Conv1D(filters=256, kernel_size=15, strides=2, padding='same')(drop4)\nbatch4 = BatchNormalization()(conv4)\nrelu4 = ReLU()(batch4)\ndrop4 = Dropout(rate=0.1)(relu4)\n\n# Shortcut connection\nmax3 = MaxPooling1D(pool_size=5, strides=2, padding='same')(conc2)\nconv_ = Conv1D(256, kernel_size=1, strides=1, padding='same')(max3)\nconc3 = Add()([conv4, conv_])\n\n# Final convolutional block\nbatch5 = BatchNormalization()(conc3)\nrelu5 = ReLU()(batch5)\n\n# Global pooling and fully connected layers\nX = GlobalAveragePooling1D()(relu5)\n\nX = Dense(units=128, kernel_regularizer=tf.keras.regularizers.L2(0.005))(X)\nX = BatchNormalization()(X)\nX = ReLU()(X)\nX = Dropout(rate=0.1)(X)\n\nX = Dense(units=64, kernel_regularizer=tf.keras.regularizers.L2(0.009))(X)\nX = BatchNormalization()(X)\nX = ReLU()(X)\nX = Dropout(rate=0.15)(X)\n\n# Output layer\noutput = Dense(no_of_classes, activation='sigmoid')(X)\n\n# Define the model\nmodel = Model(inputs=input, outputs=output)\n\n# Compile the model\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n    loss=tf.keras.losses.BinaryCrossentropy(),\n    metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC(curve='ROC', multi_label=True)]\n)\n\nprint(model.summary())\n\n\n\"\"\"#### Train Model\"\"\"\nimport tensorflow as tf \nearly    = callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True)\nreducelr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=3)\n\ncallback = [early, reducelr]\n\n\nmodel.compile(optimizer = optimizers.Adam(learning_rate=0.0005),\n              loss = losses.BinaryCrossentropy(),\n              metrics = [metrics.BinaryAccuracy(), metrics.AUC(curve='ROC', multi_label=True)])\n\nhistory = model.fit(x_train, y_train, validation_split=0.12, epochs=100, batch_size=32, callbacks=callback)\n\n\n# \"\"\"##### Save Model\"\"\"\n\nsave_path = 'save_path/'\nmodel.save(save_path + \"First_Paper.h5\")\n\n\n\n\"\"\"Evaluate the model\"\"\"\n\nfrom sklearn.metrics import precision_recall_curve, f1_score, roc_auc_score, accuracy_score, auc\n\n\ndef sklearn_metrics(y_true, y_pred):\n    y_bin = np.copy(y_pred)\n    y_bin[y_bin >= 0.5] = 1\n    y_bin[y_bin < 0.5]  = 0\n\n    # Compute area under precision-Recall curve\n    auc_sum = 0\n    for i in range(no_of_classes):\n      precision, recall, thresholds = precision_recall_curve(y_true[:, i], y_pred[:,i])\n      auc_sum += auc(recall, precision)\n\n    print(\"Accuracy        : {:.2f}\".format(accuracy_score(y_true.flatten(), y_bin.flatten())* 100))\n    print(\"Macro AUC score : {:.2f}\".format(roc_auc_score(y_true, y_pred, average='macro') * 100))\n    print('AUPRC           : {:.2f}'.format((auc_sum / no_of_classes) * 100))\n    print(\"Micro F1 score  : {:.2f}\".format(f1_score(y_true, y_bin, average='micro') * 100))\n\n\n\nfrom typing import Tuple\nimport numpy as np\nimport os\n\nimport numpy as np\nimport warnings\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\n\ndef Metrics(y_true: np.ndarray, y_scores: np.ndarray) -> Tuple[float, float]:\n    \"\"\"Metrics for class-wise accuracy and mean accuracy.\n\n    Parameters\n    ----------\n    y_true : np.ndarray\n        Ground truth labels.\n    y_scores : np.ndarray\n        Predicted labels.\n\n    Returns\n    -------\n    tuple[np.ndarray]\n        Tuple of arrays containing class-wise accuracy and mean accuracy.\n\n    \"\"\"\n\n    y_pred = y_scores >= 0.5\n    acc = np.zeros(y_pred.shape[-1])\n\n    for i in range(y_pred.shape[-1]):\n        acc[i] = accuracy_score(y_true[:, i], y_pred[:, i])\n\n    return acc.tolist(), np.mean(acc)\n\n\ndef AUC(y_true: np.ndarray, y_pred: np.ndarray, verbose: bool = False) -> float:\n    \"\"\"Computes the macro-averaged AUC score.\n\n    Parameters\n    ----------\n    y_true : np.ndarray\n        Ground truth labels.\n    y_scores : np.ndarray\n        Predicted probabilities.\n\n    Returns\n    -------\n    float\n        macro-average AUC score.\n\n    \"\"\"\n\n    aucs = []\n    assert (\n        len(y_true.shape) == 2 and len(y_pred.shape) == 2\n    ), \"Predictions and labels must be 2D.\"\n    for col in range(y_true.shape[1]):\n        try:\n            aucs.append(roc_auc_score(y_true[:, col], y_pred[:, col]))\n        except ValueError as e:\n            if verbose:\n                print(\n                    f\"Value error encountered for label {col}, likely due to using mixup or \"\n                    f\"lack of full label presence. Setting AUC to accuracy. \"\n                    f\"Original error was: {str(e)}.\"\n                )\n            aucs.append((y_pred == y_true).sum() / len(y_pred))\n    return aucs\n\n\ny_pred_train = model.predict(x_train)\ny_pred_test  = model.predict(x_test)\n\nprint(\"Train\")\nsklearn_metrics(y_train, y_pred_train)\nprint(\"\\nTest\")\nsklearn_metrics(y_test, y_pred_test)\n\nacc, mean_acc = Metrics(y_test, y_pred_test)\nclass_auc = AUC(y_test, y_pred_test)\n\nprint(f\"class wise accuracy: {acc}\")\n\nprint(f\"class wise AUC : {class_auc}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:42:15.576019Z","iopub.execute_input":"2025-05-10T14:42:15.576237Z","iopub.status.idle":"2025-05-10T14:56:20.457058Z","shell.execute_reply.started":"2025-05-10T14:42:15.576219Z","shell.execute_reply":"2025-05-10T14:56:20.456414Z"}},"outputs":[{"name":"stderr","text":"2025-05-10 14:42:17.496093: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746888137.730210      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746888137.790651      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Original shapes:\nx_train: (19634, 5000, 1)\nx_test: (2203, 5000, 1)\nAfter transpose:\nx_train: (19634, 5000, 1)\nx_test: (2203, 5000, 1)\nUsing 19634 training samples and 2203 test samples\nFinal shapes:\nx_train: (19634, 5000, 1)\nx_test: (2203, 5000, 1)\nx_train : (19634, 5000, 1)\ny_train : (19634, 5)\nx_test  : (2203, 5000, 1)\ny_test  : (2203, 5)\nData loaded\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1746888151.377052      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1746888151.377813      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m1\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m512\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │            \u001b[38;5;34m128\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ re_lu (\u001b[38;5;33mReLU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5000\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization[\u001b[38;5;34m0\u001b[0m… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling1d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2500\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ re_lu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2500\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │         \u001b[38;5;34m30,784\u001b[0m │ re_lu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2500\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │          \u001b[38;5;34m2,112\u001b[0m │ max_pooling1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add (\u001b[38;5;33mAdd\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2500\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│                           │                        │                │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2500\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │            \u001b[38;5;34m256\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ re_lu_2 (\u001b[38;5;33mReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2500\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_2… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2500\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ re_lu_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling1d_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m123,008\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │          \u001b[38;5;34m8,320\u001b[0m │ max_pooling1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_1 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│                           │                        │                │ conv1d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_4     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ re_lu_4 (\u001b[38;5;33mReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_4… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1250\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ re_lu_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling1d_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m625\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m625\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │        \u001b[38;5;34m491,776\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m625\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │         \u001b[38;5;34m33,024\u001b[0m │ max_pooling1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_2 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m625\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ conv1d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│                           │                        │                │ conv1d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_6     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m625\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m1,024\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ re_lu_6 (\u001b[38;5;33mReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m625\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_6… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ re_lu_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m32,896\u001b[0m │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_7     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │            \u001b[38;5;34m512\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ re_lu_7 (\u001b[38;5;33mReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_7… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ re_lu_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m8,256\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_8     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │            \u001b[38;5;34m256\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ re_lu_8 (\u001b[38;5;33mReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_8… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ re_lu_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m325\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling1d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ re_lu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">30,784</span> │ re_lu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│                           │                        │                │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_2… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ re_lu_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling1d_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">123,008</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ max_pooling1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│                           │                        │                │ conv1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_4     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_4… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ re_lu_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling1d_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">625</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">625</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">491,776</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">625</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ max_pooling1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">625</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│                           │                        │                │ conv1d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_6     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">625</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ re_lu_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">625</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_6… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ re_lu_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_7     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ re_lu_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_7… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ re_lu_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ batch_normalization_8     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ re_lu_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_8… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ re_lu_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m733,701\u001b[0m (2.80 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">733,701</span> (2.80 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m732,357\u001b[0m (2.79 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">732,357</span> (2.79 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,344\u001b[0m (5.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> (5.25 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1746888162.252032     108 service.cc:148] XLA service 0x78c87c003a60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1746888162.252778     108 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1746888162.252798     108 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1746888163.112454     108 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  2/540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 74ms/step - auc_1: 0.4199 - binary_accuracy: 0.4672 - loss: 2.4759   ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1746888171.884690     108 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 66ms/step - auc_1: 0.6784 - binary_accuracy: 0.7200 - loss: 1.5697 - val_auc_1: 0.7445 - val_binary_accuracy: 0.7666 - val_loss: 0.6885 - learning_rate: 5.0000e-04\nEpoch 2/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 46ms/step - auc_1: 0.7810 - binary_accuracy: 0.8097 - loss: 0.5611 - val_auc_1: 0.7881 - val_binary_accuracy: 0.7567 - val_loss: 0.5367 - learning_rate: 5.0000e-04\nEpoch 3/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 47ms/step - auc_1: 0.7963 - binary_accuracy: 0.8145 - loss: 0.4543 - val_auc_1: 0.8019 - val_binary_accuracy: 0.7982 - val_loss: 0.4604 - learning_rate: 5.0000e-04\nEpoch 4/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 49ms/step - auc_1: 0.7995 - binary_accuracy: 0.8173 - loss: 0.4328 - val_auc_1: 0.8053 - val_binary_accuracy: 0.7580 - val_loss: 0.5681 - learning_rate: 5.0000e-04\nEpoch 5/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 49ms/step - auc_1: 0.8117 - binary_accuracy: 0.8216 - loss: 0.4200 - val_auc_1: 0.8119 - val_binary_accuracy: 0.7943 - val_loss: 0.4514 - learning_rate: 5.0000e-04\nEpoch 6/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 48ms/step - auc_1: 0.8115 - binary_accuracy: 0.8212 - loss: 0.4168 - val_auc_1: 0.8150 - val_binary_accuracy: 0.8053 - val_loss: 0.4424 - learning_rate: 5.0000e-04\nEpoch 7/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8105 - binary_accuracy: 0.8205 - loss: 0.4177 - val_auc_1: 0.7358 - val_binary_accuracy: 0.7530 - val_loss: 0.5778 - learning_rate: 5.0000e-04\nEpoch 8/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8175 - binary_accuracy: 0.8248 - loss: 0.4101 - val_auc_1: 0.7965 - val_binary_accuracy: 0.8030 - val_loss: 0.4486 - learning_rate: 5.0000e-04\nEpoch 9/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8178 - binary_accuracy: 0.8253 - loss: 0.4101 - val_auc_1: 0.8050 - val_binary_accuracy: 0.6882 - val_loss: 0.6073 - learning_rate: 5.0000e-04\nEpoch 10/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8266 - binary_accuracy: 0.8300 - loss: 0.4007 - val_auc_1: 0.8339 - val_binary_accuracy: 0.8145 - val_loss: 0.4160 - learning_rate: 5.0000e-05\nEpoch 11/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 48ms/step - auc_1: 0.8386 - binary_accuracy: 0.8342 - loss: 0.3879 - val_auc_1: 0.8435 - val_binary_accuracy: 0.8230 - val_loss: 0.3961 - learning_rate: 5.0000e-05\nEpoch 12/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8421 - binary_accuracy: 0.8369 - loss: 0.3847 - val_auc_1: 0.8457 - val_binary_accuracy: 0.8243 - val_loss: 0.3973 - learning_rate: 5.0000e-05\nEpoch 13/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8426 - binary_accuracy: 0.8346 - loss: 0.3827 - val_auc_1: 0.8441 - val_binary_accuracy: 0.8232 - val_loss: 0.3981 - learning_rate: 5.0000e-05\nEpoch 14/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8407 - binary_accuracy: 0.8355 - loss: 0.3833 - val_auc_1: 0.8454 - val_binary_accuracy: 0.8203 - val_loss: 0.3977 - learning_rate: 5.0000e-05\nEpoch 15/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8434 - binary_accuracy: 0.8359 - loss: 0.3801 - val_auc_1: 0.8478 - val_binary_accuracy: 0.8262 - val_loss: 0.3920 - learning_rate: 5.0000e-06\nEpoch 16/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8487 - binary_accuracy: 0.8385 - loss: 0.3758 - val_auc_1: 0.8494 - val_binary_accuracy: 0.8238 - val_loss: 0.3894 - learning_rate: 5.0000e-06\nEpoch 17/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8474 - binary_accuracy: 0.8379 - loss: 0.3750 - val_auc_1: 0.8477 - val_binary_accuracy: 0.8255 - val_loss: 0.3916 - learning_rate: 5.0000e-06\nEpoch 18/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8498 - binary_accuracy: 0.8379 - loss: 0.3745 - val_auc_1: 0.8511 - val_binary_accuracy: 0.8272 - val_loss: 0.3859 - learning_rate: 5.0000e-06\nEpoch 19/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8437 - binary_accuracy: 0.8398 - loss: 0.3758 - val_auc_1: 0.8489 - val_binary_accuracy: 0.8263 - val_loss: 0.3900 - learning_rate: 5.0000e-06\nEpoch 20/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8467 - binary_accuracy: 0.8362 - loss: 0.3798 - val_auc_1: 0.8500 - val_binary_accuracy: 0.8246 - val_loss: 0.3880 - learning_rate: 5.0000e-06\nEpoch 21/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8492 - binary_accuracy: 0.8406 - loss: 0.3737 - val_auc_1: 0.8468 - val_binary_accuracy: 0.8234 - val_loss: 0.3922 - learning_rate: 5.0000e-06\nEpoch 22/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8514 - binary_accuracy: 0.8388 - loss: 0.3728 - val_auc_1: 0.8497 - val_binary_accuracy: 0.8258 - val_loss: 0.3880 - learning_rate: 5.0000e-07\nEpoch 23/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8499 - binary_accuracy: 0.8397 - loss: 0.3725 - val_auc_1: 0.8508 - val_binary_accuracy: 0.8260 - val_loss: 0.3864 - learning_rate: 5.0000e-07\nEpoch 24/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8517 - binary_accuracy: 0.8401 - loss: 0.3723 - val_auc_1: 0.8509 - val_binary_accuracy: 0.8265 - val_loss: 0.3857 - learning_rate: 5.0000e-07\nEpoch 25/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8469 - binary_accuracy: 0.8385 - loss: 0.3754 - val_auc_1: 0.8504 - val_binary_accuracy: 0.8244 - val_loss: 0.3877 - learning_rate: 5.0000e-07\nEpoch 26/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8505 - binary_accuracy: 0.8383 - loss: 0.3734 - val_auc_1: 0.8508 - val_binary_accuracy: 0.8258 - val_loss: 0.3865 - learning_rate: 5.0000e-07\nEpoch 27/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8465 - binary_accuracy: 0.8379 - loss: 0.3782 - val_auc_1: 0.8512 - val_binary_accuracy: 0.8257 - val_loss: 0.3860 - learning_rate: 5.0000e-07\nEpoch 28/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8497 - binary_accuracy: 0.8388 - loss: 0.3725 - val_auc_1: 0.8490 - val_binary_accuracy: 0.8250 - val_loss: 0.3891 - learning_rate: 5.0000e-08\nEpoch 29/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8527 - binary_accuracy: 0.8419 - loss: 0.3708 - val_auc_1: 0.8497 - val_binary_accuracy: 0.8250 - val_loss: 0.3887 - learning_rate: 5.0000e-08\nEpoch 30/100\n\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - auc_1: 0.8503 - binary_accuracy: 0.8388 - loss: 0.3742 - val_auc_1: 0.8506 - val_binary_accuracy: 0.8264 - val_loss: 0.3866 - learning_rate: 5.0000e-08\n\u001b[1m614/614\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step\n\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step\nTrain\nAccuracy        : 84.18\nMacro AUC score : 86.14\nAUPRC           : 68.16\nMicro F1 score  : 65.36\n\nTest\nAccuracy        : 83.52\nMacro AUC score : 83.79\nAUPRC           : 64.76\nMicro F1 score  : 64.07\nclass wise accuracy: [0.8447571493418067, 0.8946890603722197, 0.7812074443940081, 0.813890149795733, 0.8415796640944168]\nclass wise AUC : [0.8197652781212827, 0.8094233859903571, 0.7906663378815278, 0.8951336072793278, 0.8744537011745424]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Save the complete model with weights, architecture, and optimizer state\nmodel_save_path = '/kaggle/working/ecg_classifier.h5'\nmodel.save(model_save_path)\n\n# Also save the multilabel binarizer for consistent label encoding/decoding\nimport pickle\nmlb_save_path = '/kaggle/working/mlb_encoder.pkl'\nwith open(mlb_save_path, 'wb') as f:\n    pickle.dump(mlb, f)\n\nprint(f\"Model saved to {model_save_path}\")\nprint(f\"MultiLabelBinarizer saved to {mlb_save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:56:20.457855Z","iopub.execute_input":"2025-05-10T14:56:20.458460Z","iopub.status.idle":"2025-05-10T14:56:20.562273Z","shell.execute_reply.started":"2025-05-10T14:56:20.458440Z","shell.execute_reply":"2025-05-10T14:56:20.561630Z"}},"outputs":[{"name":"stdout","text":"Model saved to /kaggle/working/ecg_classifier.h5\nMultiLabelBinarizer saved to /kaggle/working/mlb_encoder.pkl\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import numpy as np\nimport wfdb\nimport tensorflow as tf\nimport os\nimport pickle\nfrom sklearn.preprocessing import MultiLabelBinarizer\ndataset_path=\"/kaggle/input/ptb-xl-dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1\"\ndef predict_ecg_from_file(dat_file_path, model_path, mlb_path=None, lead_name=\"lead-I\"):\n    \"\"\"\n    Preprocess a .dat file, load the saved model, and predict ECG classes in Kaggle environment.\n    \n    Parameters:\n    -----------\n    dat_file_path : str\n        Path to the .dat file (without the .dat extension)\n    model_path : str\n        Path to the saved model (.h5 file)\n    mlb_path : str, optional\n        Path to the saved MultiLabelBinarizer pickle file for label decoding\n    lead_name : str, optional\n        Lead configuration to use, default is \"lead-I\"\n        \n    Returns:\n    --------\n    dict\n        Dictionary containing prediction probabilities and class names if mlb_path is provided\n    \"\"\"\n    # Define lead types mapping (same as in your training code)\n    lead_types = {\n        \"lead-I\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n        \"bipolar-limb\": [3, 4, 5, 6, 7, 8, 9, 10, 11],\n        \"unipolar-limb\": [0, 1, 2, 6, 7, 8, 9, 10, 11],\n        \"limb-leads\": [6, 7, 8, 9, 10, 11],\n        \"precordial-leads\": [0, 1, 2, 3, 4, 5],\n        \"all-lead\": [],\n    }\n    \n    # Step 1: Load the raw ECG data\n    try:\n        # Read the signal file (wfdb expects path without extension)\n        signal, meta = wfdb.rdsamp(dat_file_path)\n        print(f\"Loaded signal with shape: {signal.shape}\")\n        \n        # Step 2: Preprocess the data\n        # Remove unwanted leads according to lead_name configuration\n        if lead_types[lead_name]:\n            signal = np.delete(signal, lead_types[lead_name], axis=1)\n            print(f\"After lead selection: {signal.shape}\")\n        \n        # Reshape to match model input requirements\n        # Assuming fixed length of 5000 time points\n        if signal.shape[0] != 5000:\n            print(f\"Warning: Signal length is {signal.shape[0]}, expected 5000. Resampling...\")\n            # Resample to 5000 points if needed (simple interpolation)\n            from scipy.interpolate import interp1d\n            x = np.linspace(0, 1, signal.shape[0])\n            x_new = np.linspace(0, 1, 5000)\n            f = interp1d(x, signal, axis=0, kind='linear')\n            signal = f(x_new)\n        \n        # Reshape for model input (1, 5000, n_leads)\n        X = signal.transpose(1, 0).reshape(1, 5000, -1)\n        print(f\"Final input shape: {X.shape}\")\n        \n        # Step 3: Load the model\n        model = tf.keras.models.load_model(model_path)\n        print(\"Model loaded successfully\")\n        \n        # Step 4: Make predictions\n        predictions = model.predict(X)\n        \n        # Step 5: Process results\n        result = {\"raw_predictions\": predictions[0].tolist()}\n        \n        # If mlb_path is provided, decode the labels\n        if mlb_path and os.path.exists(mlb_path):\n            with open(mlb_path, 'rb') as f:\n                mlb = pickle.load(f)\n            \n            # Get classes with probability > 0.5\n            binary_preds = (predictions[0] >= 0.5).astype(int)\n            predicted_classes = mlb.inverse_transform(binary_preds.reshape(1, -1))[0]\n            \n            # Add class probabilities to result\n            class_probs = {}\n            for i, class_name in enumerate(mlb.classes_):\n                class_probs[class_name] = float(predictions[0][i])\n            \n            result[\"predicted_classes\"] = list(predicted_classes)\n            result[\"class_probabilities\"] = class_probs\n        \n        return result\n        \n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"error\": str(e)}\n\n# This function creates an example to test the model with an example from the dataset\ndef test_with_dataset_example(dataset_path, model_path, mlb_path=None, sample_idx=0):\n    \"\"\"\n    Load a sample from the original dataset to test the model\n    \n    Parameters:\n    -----------\n    dataset_path : str\n        Path to the PTB-XL dataset\n    model_path : str\n        Path to the saved model\n    mlb_path : str, optional\n        Path to the saved MultiLabelBinarizer\n    sample_idx : int, optional\n        Index of the sample to use from the test set\n        \n    Returns:\n    --------\n    dict\n        Prediction results and ground truth\n    \"\"\"\n    import pandas as pd\n    import ast\n    \n    # Load metadata\n    meta_path = os.path.join(dataset_path, 'ptbxl_database.csv')\n    Y = pd.read_csv(meta_path, index_col='ecg_id')\n    Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\n    \n    # Load SCP statements for diagnostic classes\n    agg_path = os.path.join(dataset_path, 'scp_statements.csv')\n    agg_df = pd.read_csv(agg_path, index_col=0)\n    agg_df = agg_df[agg_df.diagnostic == 1]\n    \n    # Get test data (fold 10)\n    test_fold = 10\n    test_data = Y[Y.strat_fold == test_fold]\n    \n    if len(test_data) == 0:\n        return {\"error\": \"No test data found\"}\n    \n    # Select a sample\n    if sample_idx >= len(test_data):\n        sample_idx = 0\n        \n    sample = test_data.iloc[sample_idx]\n    file_path = os.path.join(dataset_path, sample.filename_hr)\n    \n    # Get ground truth labels (superclasses)\n    def aggregate_diagnostic(y_dic):\n        return list(set(agg_df.loc[key].diagnostic_class for key in y_dic.keys() \n                      if key in agg_df.index))\n                      \n    true_labels = aggregate_diagnostic(sample.scp_codes)\n    \n    print(f\"Testing with example: {file_path}\")\n    print(f\"True labels: {true_labels}\")\n    \n    # Get prediction\n    prediction = predict_ecg_from_file(file_path, model_path, mlb_path)\n    prediction[\"true_labels\"] = true_labels\n    \n    return prediction\n\n# Example usage for Kaggle notebook - just copy and modify this code in your notebook\ndef example_usage():\n    \"\"\"\n    Example of how to use the function in a Kaggle notebook\n    \"\"\"\n    # Kaggle paths\n    model_path = '/kaggle/working/ecg_classifier.h5'\n    mlb_path = '/kaggle/working/mlb_encoder.pkl'\n    \n    # Option 1: Test with a sample from the original dataset\n    results = test_with_dataset_example(dataset_path, model_path, mlb_path,967)\n    \n    # Option 2: If you have a specific .dat file to test\n    # specific_file = '/kaggle/input/test-file/calibrated_ecg.dat'  # path without .dat extension\n    # results = predict_ecg_from_file(specific_file, model_path, mlb_path)\n    \n    # Print results\n    print(\"\\n----- PREDICTION RESULTS -----\")\n    if \"error\" in results:\n        print(f\"Error: {results['error']}\")\n    else:\n        if \"true_labels\" in results:\n            print(f\"True labels: {results['true_labels']}\")\n            \n        if \"predicted_classes\" in results:\n            print(f\"Predicted classes: {results['predicted_classes']}\")\n            \n            # Show probabilities of all classes\n            print(\"\\nClass probabilities:\")\n            for cls, prob in sorted(results[\"class_probabilities\"].items(), \n                                   key=lambda x: x[1], reverse=True):\n                print(f\"- {cls}: {prob:.4f}\")\n        else:\n            print(\"Raw predictions:\", results[\"raw_predictions\"])\n            \nexample_usage()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:56:20.563523Z","iopub.execute_input":"2025-05-10T14:56:20.563738Z","iopub.status.idle":"2025-05-10T14:56:22.376325Z","shell.execute_reply.started":"2025-05-10T14:56:20.563721Z","shell.execute_reply":"2025-05-10T14:56:22.375749Z"}},"outputs":[{"name":"stdout","text":"Testing with example: /kaggle/input/ptb-xl-dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/records500/08000/08344_hr\nTrue labels: ['MI']\nLoaded signal with shape: (5000, 12)\nAfter lead selection: (5000, 1)\nFinal input shape: (1, 5000, 1)\nModel loaded successfully\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n\n----- PREDICTION RESULTS -----\nTrue labels: ['MI']\nPredicted classes: []\n\nClass probabilities:\n- NORM: 0.4742\n- CD: 0.3691\n- MI: 0.3162\n- HYP: 0.1057\n- STTC: 0.0925\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import numpy as np\nimport os\nimport tensorflow as tf\nimport pickle\nfrom scipy.interpolate import interp1d\nfrom collections import Counter\n\ndef read_lead_i_long_dat_file(dat_file_path, sampling_rate=500, data_format='16', scale_factor=0.001):\n    \"\"\"\n    Read a 30-second pure Lead I .dat file directly and properly scale it\n    \n    Parameters:\n    -----------\n    dat_file_path : str\n        Path to the .dat file (with or without .dat extension)\n    sampling_rate : int\n        Sampling rate in Hz (default 500Hz)\n    data_format : str\n        Data format of the binary file: '16' for 16-bit integers, '32' for 32-bit floats\n    scale_factor : float\n        Scale factor to convert units (0.001 for converting µV to mV)\n        \n    Returns:\n    --------\n    numpy.ndarray\n        ECG signal data for Lead I with shape (total_samples,)\n    \"\"\"\n    # Ensure the path ends with .dat\n    if not dat_file_path.endswith('.dat'):\n        dat_file_path += '.dat'\n    \n    # Expected samples for full 30 seconds\n    expected_samples = sampling_rate * 30\n    \n    # Read the binary data\n    try:\n        if data_format == '16':\n            # 16-bit signed integers (common format for ECG)\n            data = np.fromfile(dat_file_path, dtype=np.int16)\n        elif data_format == '32':\n            # 32-bit floating point (less common)\n            data = np.fromfile(dat_file_path, dtype=np.float32)\n        else:\n            raise ValueError(f\"Unsupported data format: {data_format}\")\n    \n        print(f\"Read {len(data)} data points from file\")\n        \n        # For pure Lead I data, no need to reshape or extract\n        print(f\"Processing pure Lead I data (applying scaling factor {scale_factor})\")\n        \n        # Apply scaling to convert µV to mV\n        signal = data * scale_factor\n        print(f\"Signal amplitude range after scaling: {np.min(signal):.4f} to {np.max(signal):.4f}\")\n        \n        # Handle if signal is not exactly 30 seconds\n        if len(signal) < expected_samples:\n            print(f\"Warning: Signal is shorter than expected 30 seconds ({len(signal)} samples)\")\n            # Pad with zeros if too short\n            padded_signal = np.zeros(expected_samples)\n            padded_signal[:len(signal)] = signal\n            signal = padded_signal\n        elif len(signal) > expected_samples:\n            print(f\"Warning: Signal is longer than expected 30 seconds ({len(signal)} samples)\")\n            # Truncate if too long\n            signal = signal[:expected_samples]\n        \n        return signal\n        \n    except Exception as e:\n        print(f\"Error reading data file: {e}\")\n        raise\n\ndef segment_signal(signal, sampling_rate=500):\n    \"\"\"\n    Segment a 30-second signal into three 10-second segments\n    \n    Parameters:\n    -----------\n    signal : numpy.ndarray\n        The full signal to segment\n    sampling_rate : int\n        Sampling rate in Hz\n        \n    Returns:\n    --------\n    list\n        List of three 10-second signal segments\n    \"\"\"\n    # Calculate samples per segment (10 seconds)\n    segment_samples = sampling_rate * 10\n    \n    # Expected samples for full 30 seconds\n    expected_samples = sampling_rate * 30\n    \n    # Ensure the signal is 30 seconds long\n    if len(signal) != expected_samples:\n        print(f\"Warning: Signal length is {len(signal)}, expected {expected_samples}. Reshaping...\")\n        # Resample to 30 seconds\n        x = np.linspace(0, 1, len(signal))\n        x_new = np.linspace(0, 1, expected_samples)\n        f = interp1d(x, signal, kind='linear', bounds_error=False, fill_value=\"extrapolate\")\n        signal = f(x_new)\n    \n    # Split the signal into three 10-second segments\n    segments = []\n    for i in range(3):\n        start_idx = i * segment_samples\n        end_idx = (i + 1) * segment_samples\n        segment = signal[start_idx:end_idx]\n        segments.append(segment)\n        \n    return segments\n\ndef process_segment(segment, sampling_rate=500):\n    \"\"\"\n    Process a segment of ECG data to ensure it's properly formatted for the model\n    \n    Parameters:\n    -----------\n    segment : numpy.ndarray\n        Raw ECG segment\n    sampling_rate : int\n        Sampling rate of the ECG\n        \n    Returns:\n    --------\n    numpy.ndarray\n        Processed segment ready for model input\n    \"\"\"\n    # Ensure correct length (5000 samples for 10 seconds)\n    if len(segment) != 5000:\n        x = np.linspace(0, 1, len(segment))\n        x_new = np.linspace(0, 1, 5000)\n        f = interp1d(x, segment, kind='linear', bounds_error=False, fill_value=\"extrapolate\")\n        segment = f(x_new)\n    \n    # Optional: Apply additional preprocessing if needed for your model\n    # For example, if your model was trained on normalized data:\n    # if np.std(segment) > 0:\n    #     segment = (segment - np.mean(segment)) / np.std(segment)\n    \n    return segment\n\ndef predict_with_voting(dat_file_path, model_path, mlb_path=None, sampling_rate=500, scale_factor=0.001):\n    \"\"\"\n    Process a 30-second .dat file, properly scale it, split it into three 10-second segments,\n    make predictions on each segment, and determine the final result through voting.\n    \n    Parameters:\n    -----------\n    dat_file_path : str\n        Path to the .dat file\n    model_path : str\n        Path to the saved model (.h5 file)\n    mlb_path : str, optional\n        Path to the saved MultiLabelBinarizer pickle file for label decoding\n    sampling_rate : int\n        Sampling rate in Hz (default 500Hz)\n    scale_factor : float\n        Scale factor to convert units (0.001 for converting µV to mV)\n        \n    Returns:\n    --------\n    dict\n        Dictionary containing voting results, segment predictions, and final class probabilities\n    \"\"\"\n    try:\n        # Step 1: Read the 30-second ECG data (pure Lead I) and apply scaling\n        full_signal = read_lead_i_long_dat_file(\n            dat_file_path, \n            sampling_rate=sampling_rate,\n            scale_factor=scale_factor\n        )\n        print(f\"Loaded 30-second Lead I signal with {len(full_signal)} samples\")\n        \n        # Step 2: Split into three 10-second segments\n        segments = segment_signal(full_signal, sampling_rate)\n        print(f\"Split into {len(segments)} segments of {len(segments[0])} samples each\")\n        \n        # Step 3: Load the model (load once to improve performance)\n        model = tf.keras.models.load_model(model_path)\n        print(\"Model loaded successfully\")\n        \n        # Load MLB if provided\n        mlb = None\n        if mlb_path and os.path.exists(mlb_path):\n            with open(mlb_path, 'rb') as f:\n                mlb = pickle.load(f)\n        \n        # Step 4: Process each segment and collect predictions\n        segment_results = []\n        all_predictions = []\n        all_predicted_classes = []\n        \n        for i, segment in enumerate(segments):\n            print(f\"Processing segment {i+1}/3...\")\n            \n            # Process the segment to ensure it's properly formatted\n            processed_segment = process_segment(segment)\n            \n            # Reshape for model input (batch, time, channels)\n            X = processed_segment.reshape(1, 5000, 1)\n            \n            # Make predictions\n            predictions = model.predict(X, verbose=0)\n            all_predictions.append(predictions[0])\n            \n            # Process segment results\n            segment_result = {\"raw_predictions\": predictions[0].tolist()}\n            \n            # Decode labels if MLB is provided\n            if mlb is not None:\n                # Get classes with probability > 0.5\n                binary_preds = (predictions[0] >= 0.5).astype(int)\n                predicted_classes = mlb.inverse_transform(binary_preds.reshape(1, -1))[0]\n                all_predicted_classes.append(set(predicted_classes))\n                \n                # Add class probabilities\n                class_probs = {}\n                for j, class_name in enumerate(mlb.classes_):\n                    class_probs[class_name] = float(predictions[0][j])\n                \n                segment_result[\"predicted_classes\"] = list(predicted_classes)\n                segment_result[\"class_probabilities\"] = class_probs\n            \n            segment_results.append(segment_result)\n        \n        # Step 5: Implement voting mechanism\n        final_result = {\"segment_results\": segment_results}\n        \n        # Average the raw predictions\n        avg_predictions = np.mean(all_predictions, axis=0)\n        final_result[\"averaged_raw_predictions\"] = avg_predictions.tolist()\n        \n        # Implement voting for class labels\n        if mlb is not None:\n            # Flatten all predicted classes into a single list\n            all_classes = []\n            for classes in all_predicted_classes:\n                all_classes.extend(classes)\n            \n            # Count occurrences of each class\n            class_counts = Counter(all_classes)\n            \n            # Classes that appear in at least 2 of 3 segments (majority vote)\n            voted_classes = [cls for cls, count in class_counts.items() if count >= 2]\n            \n            # If no classes meet the threshold, take the ones with highest average probability\n            if not voted_classes:\n                # Calculate average probability for each class\n                avg_class_probs = {}\n                for cls_idx, cls_name in enumerate(mlb.classes_):\n                    avg_prob = np.mean([pred[cls_idx] for pred in all_predictions])\n                    avg_class_probs[cls_name] = avg_prob\n                \n                # Take classes with average probability > 0.5\n                voted_classes = [cls for cls, prob in avg_class_probs.items() if prob >= 0.5]\n                \n                # If still no classes, take the top one\n                if not voted_classes:\n                    voted_classes = [max(avg_class_probs.items(), key=lambda x: x[1])[0]]\n            \n            # Calculate final class probabilities (average across segments)\n            final_class_probs = {}\n            for cls_idx, cls_name in enumerate(mlb.classes_):\n                final_class_probs[cls_name] = float(np.mean([pred[cls_idx] for pred in all_predictions]))\n            \n            final_result[\"voted_classes\"] = voted_classes\n            final_result[\"final_class_probabilities\"] = final_class_probs\n        \n        return final_result\n        \n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"error\": str(e)}\n\ndef visualize_segment(signal, sampling_rate=500, segment_num=None):\n    \"\"\"\n    Helper function to visualize a segment of ECG data\n    \n    Parameters:\n    -----------\n    signal : numpy.ndarray\n        The ECG signal to visualize\n    sampling_rate : int\n        Sampling rate of the ECG\n    segment_num : int, optional\n        Segment number for title\n    \"\"\"\n    try:\n        import matplotlib.pyplot as plt\n        \n        # Time axis in seconds\n        time = np.arange(len(signal)) / sampling_rate\n        \n        plt.figure(figsize=(15, 4))\n        plt.plot(time, signal)\n        title = \"ECG Signal\"\n        if segment_num is not None:\n            title += f\" - Segment {segment_num}\"\n        plt.title(title)\n        plt.xlabel(\"Time (seconds)\")\n        plt.ylabel(\"Amplitude (mV)\")\n        plt.grid(True)\n        \n        filename = f\"/kaggle/working/ecg_segment_{segment_num}.png\" if segment_num else \"/kaggle/working/ecg_full.png\"\n        plt.savefig(filename)\n        plt.close()\n        print(f\"Visualization saved to {filename}\")\n        \n    except Exception as e:\n        print(f\"Error visualizing signal: {e}\")\n\n# Main execution for Kaggle environment\nif __name__ == \"__main__\":\n    # Specific paths for Kaggle\n    dat_file_path = '/kaggle/input/test-file/calibrated_ecg.dat'\n    model_path = '/kaggle/working/ecg_classifier.h5'\n    mlb_path = '/kaggle/working/mlb_encoder.pkl'\n    \n    print(\"Starting ECG analysis...\")\n    print(f\"DAT file: {dat_file_path}\")\n    print(f\"Model path: {model_path}\")\n    print(f\"MLB path: {mlb_path}\")\n    \n    # Get predictions with voting - now with proper scaling from µV to mV\n    results = predict_with_voting(\n        dat_file_path, \n        model_path, \n        mlb_path,\n        scale_factor=0.001  # Convert microvolts to millivolts\n    )\n    \n    # Print results\n    print(\"\\n----- PREDICTION RESULTS WITH VOTING -----\")\n    if \"error\" in results:\n        print(f\"Error: {results['error']}\")\n    else:\n        if \"voted_classes\" in results:\n            print(f\"\\nFINAL PREDICTION (VOTED): {results['voted_classes']}\")\n            \n            # Show probabilities of all classes (sorted by probability)\n            print(\"\\nFinal Class Probabilities (averaged across segments):\")\n            for cls, prob in sorted(results[\"final_class_probabilities\"].items(), \n                                    key=lambda x: x[1], reverse=True):\n                print(f\"- {cls}: {prob:.4f}\")\n            \n            # Show segment-by-segment results\n            print(\"\\nResults by segment:\")\n            for i, segment_result in enumerate(results[\"segment_results\"]):\n                print(f\"\\nSegment {i+1}:\")\n                if \"predicted_classes\" in segment_result:\n                    if segment_result[\"predicted_classes\"]:\n                        print(f\"Classes: {segment_result['predicted_classes']}\")\n                    else:\n                        print(\"No classes exceeded the threshold\")\n                    \n                    # Show top probabilities for each segment\n                    print(\"Top probabilities:\")\n                    top_probs = sorted(segment_result[\"class_probabilities\"].items(), \n                                      key=lambda x: x[1], reverse=True)[:3]\n                    for cls, prob in top_probs:\n                        print(f\"- {cls}: {prob:.4f}\")\n                else:\n                    print(\"No class probabilities available\")\n        else:\n            print(\"Raw predictions (averaged):\", results[\"averaged_raw_predictions\"])\n            \n    print(\"\\nAnalysis complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:58:34.562821Z","iopub.execute_input":"2025-05-10T14:58:34.563166Z","iopub.status.idle":"2025-05-10T14:58:35.682596Z","shell.execute_reply.started":"2025-05-10T14:58:34.563127Z","shell.execute_reply":"2025-05-10T14:58:35.681819Z"}},"outputs":[{"name":"stdout","text":"Starting ECG analysis...\nDAT file: /kaggle/input/test-file/calibrated_ecg.dat\nModel path: /kaggle/working/ecg_classifier.h5\nMLB path: /kaggle/working/mlb_encoder.pkl\nRead 15000 data points from file\nProcessing pure Lead I data (applying scaling factor 0.001)\nSignal amplitude range after scaling: -0.7330 to 1.5470\nLoaded 30-second Lead I signal with 15000 samples\nSplit into 3 segments of 5000 samples each\nModel loaded successfully\nProcessing segment 1/3...\nProcessing segment 2/3...\nProcessing segment 3/3...\n\n----- PREDICTION RESULTS WITH VOTING -----\n\nFINAL PREDICTION (VOTED): ['CD']\n\nFinal Class Probabilities (averaged across segments):\n- CD: 0.3410\n- NORM: 0.2947\n- HYP: 0.2457\n- MI: 0.1499\n- STTC: 0.1111\n\nResults by segment:\n\nSegment 1:\nNo classes exceeded the threshold\nTop probabilities:\n- CD: 0.4235\n- NORM: 0.2705\n- HYP: 0.2462\n\nSegment 2:\nNo classes exceeded the threshold\nTop probabilities:\n- NORM: 0.3566\n- HYP: 0.2556\n- CD: 0.2008\n\nSegment 3:\nNo classes exceeded the threshold\nTop probabilities:\n- CD: 0.3988\n- NORM: 0.2569\n- HYP: 0.2354\n\nAnalysis complete!\n","output_type":"stream"}],"execution_count":8}]}