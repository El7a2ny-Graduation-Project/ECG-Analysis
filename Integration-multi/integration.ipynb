{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95f09be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ranaa\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnosis: NORM \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ECG Analysis Pipeline: From PDF to Diagnosis\n",
    "-------------------------------------------\n",
    "This module provides functions to:\n",
    "1. Digitize ECG from PDF files\n",
    "2. Process the digitized ECG signal\n",
    "3. Make diagnoses using a trained model\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from scipy.interpolate import interp1d\n",
    "from collections import Counter\n",
    "from pdf2image import convert_from_path\n",
    "import matplotlib.pyplot as plt  # Added for visualization\n",
    "\n",
    "def digitize_ecg_from_pdf(pdf_path, output_file='calibrated_ecg.dat', debug=False):\n",
    "    \"\"\"\n",
    "    Process an ECG PDF file and convert it to a .dat signal file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the ECG PDF file\n",
    "        output_file (str): Path to save the output .dat file (default: 'calibrated_ecg.dat')\n",
    "        debug (bool): Whether to print debug information\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the created .dat file\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"Starting ECG digitization from PDF: {pdf_path}\")\n",
    "    \n",
    "    # Convert PDF to image\n",
    "    images = convert_from_path(pdf_path)\n",
    "    temp_image_path = 'temp_ecg_image.jpg'\n",
    "    images[0].save(temp_image_path, 'JPEG')\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Converted PDF to image: {temp_image_path}\")\n",
    "    \n",
    "    # Load the image\n",
    "    img = cv2.imread(temp_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    height, width = img.shape\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Image dimensions: {width}x{height}\")\n",
    "    \n",
    "    # Fixed calibration parameters\n",
    "    calibration = {\n",
    "        'seconds_per_pixel': 2.0 / 197.0,  # 197 pixels = 2 seconds\n",
    "        'mv_per_pixel': 1.0 / 78.8,        # 78.8 pixels = 1 mV\n",
    "    }\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Calibration parameters: {calibration}\")\n",
    "    \n",
    "    # Calculate layer boundaries using percentages\n",
    "    layer1_start = int(height * 35.35 / 100)\n",
    "    layer1_end = int(height * 51.76 / 100)\n",
    "    layer2_start = int(height * 51.82 / 100)\n",
    "    layer2_end = int(height * 69.41 / 100)\n",
    "    layer3_start = int(height * 69.47 / 100)\n",
    "    layer3_end = int(height * 87.06 / 100)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Layer 1 boundaries: {layer1_start}-{layer1_end}\")\n",
    "        print(f\"Layer 2 boundaries: {layer2_start}-{layer2_end}\")\n",
    "        print(f\"Layer 3 boundaries: {layer3_start}-{layer3_end}\")\n",
    "    \n",
    "    # Crop each layer\n",
    "    layers = [\n",
    "        img[layer1_start:layer1_end, :],  # Layer 1\n",
    "        img[layer2_start:layer2_end, :],  # Layer 2\n",
    "        img[layer3_start:layer3_end, :]   # Layer 3\n",
    "    ]\n",
    "    \n",
    "    # Process each layer to extract waveform contours\n",
    "    signals = []\n",
    "    time_points = []\n",
    "    layer_duration = 10.0  # Each layer is 10 seconds long\n",
    "    \n",
    "    for i, layer in enumerate(layers):\n",
    "        if debug:\n",
    "            print(f\"Processing layer {i+1}...\")\n",
    "        \n",
    "        # Binary thresholding\n",
    "        _, binary = cv2.threshold(layer, 200, 255, cv2.THRESH_BINARY_INV)\n",
    "        \n",
    "        # Detect contours\n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        waveform_contour = max(contours, key=cv2.contourArea)  # Largest contour is the ECG\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"  - Found {len(contours)} contours\")\n",
    "            print(f\"  - Selected contour with {len(waveform_contour)} points\")\n",
    "        \n",
    "        # Sort contour points and extract coordinates\n",
    "        sorted_contour = sorted(waveform_contour, key=lambda p: p[0][0])\n",
    "        x_coords = np.array([point[0][0] for point in sorted_contour])\n",
    "        y_coords = np.array([point[0][1] for point in sorted_contour])\n",
    "        \n",
    "        # Calculate isoelectric line (one-third from the bottom)\n",
    "        isoelectric_line_y = layer.shape[0] * 0.6\n",
    "        \n",
    "        # Convert to time using fixed layer duration\n",
    "        x_min, x_max = np.min(x_coords), np.max(x_coords)\n",
    "        time = (x_coords - x_min) / (x_max - x_min) * layer_duration\n",
    "        \n",
    "        # Calculate signal in millivolts and apply baseline correction\n",
    "        signal_mv = (isoelectric_line_y - y_coords) * calibration['mv_per_pixel']\n",
    "        signal_mv = signal_mv - np.mean(signal_mv)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"  - Layer {i+1} signal range: {np.min(signal_mv):.2f} mV to {np.max(signal_mv):.2f} mV\")\n",
    "        \n",
    "        # Store the time points and calibrated signal\n",
    "        time_points.append(time)\n",
    "        signals.append(signal_mv)\n",
    "    \n",
    "    # Combine signals with proper time alignment\n",
    "    total_duration = layer_duration * len(layers)\n",
    "    sampling_frequency = 500  # Standard ECG frequency\n",
    "    num_samples = int(total_duration * sampling_frequency)\n",
    "    combined_time = np.linspace(0, total_duration, num_samples)\n",
    "    combined_signal = np.zeros(num_samples)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Combining signals with {sampling_frequency} Hz sampling rate, total duration: {total_duration}s\")\n",
    "    \n",
    "    # Place each lead at the correct time position\n",
    "    for i, (time, signal) in enumerate(zip(time_points, signals)):\n",
    "        start_time = i * layer_duration\n",
    "        mask = (combined_time >= start_time) & (combined_time < start_time + layer_duration)\n",
    "        relevant_times = combined_time[mask]\n",
    "        interpolated_signal = np.interp(relevant_times, start_time + time, signal)\n",
    "        combined_signal[mask] = interpolated_signal\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"  - Added layer {i+1} signal from {start_time}s to {start_time + layer_duration}s\")\n",
    "    \n",
    "    # Baseline correction and amplitude scaling\n",
    "    combined_signal = combined_signal - np.mean(combined_signal)\n",
    "    signal_peak = np.max(np.abs(combined_signal))\n",
    "    target_amplitude = 2.0  # Target peak amplitude in mV\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Signal peak before scaling: {signal_peak:.2f} mV\")\n",
    "    \n",
    "    if signal_peak > 0 and (signal_peak < 0.5 or signal_peak > 4.0):\n",
    "        scaling_factor = target_amplitude / signal_peak\n",
    "        combined_signal = combined_signal * scaling_factor\n",
    "        if debug:\n",
    "            print(f\"Applied scaling factor: {scaling_factor:.2f}\")\n",
    "            print(f\"Signal peak after scaling: {np.max(np.abs(combined_signal)):.2f} mV\")\n",
    "    \n",
    "    # Convert to 16-bit integers and save as .dat file\n",
    "    adc_gain = 1000.0  # Standard gain: 1000 units per mV\n",
    "    int_signal = (combined_signal * adc_gain).astype(np.int16)\n",
    "    int_signal.tofile(output_file)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Saved signal to {output_file} with {len(int_signal)} samples\")\n",
    "        print(f\"Integer signal range: {np.min(int_signal)} to {np.max(int_signal)}\")\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    if os.path.exists(temp_image_path):\n",
    "        os.remove(temp_image_path)\n",
    "        if debug:\n",
    "            print(f\"Removed temporary image: {temp_image_path}\")\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "def visualize_ecg_signal(signal, sampling_rate=500, title=\"Digitized ECG Signal\"):\n",
    "    \"\"\"\n",
    "    Visualize an ECG signal with proper time axis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    signal : numpy.ndarray\n",
    "        ECG signal data\n",
    "    sampling_rate : int\n",
    "        Sampling rate in Hz\n",
    "    title : str\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    # Calculate time axis\n",
    "    time = np.arange(len(signal)) / sampling_rate\n",
    "    \n",
    "    # Create figure with appropriate size\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(time, signal)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Amplitude (mV)')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Add 1mV scale bar\n",
    "    plt.plot([1, 1], [-0.5, 0.5], 'r-', linewidth=2)\n",
    "    plt.text(1.1, 0, '1mV', va='center')\n",
    "    \n",
    "    # Add time scale bar (1 second)\n",
    "    y_min = np.min(signal)\n",
    "    plt.plot([1, 2], [y_min, y_min], 'r-', linewidth=2)\n",
    "    plt.text(1.5, y_min - 0.1, '1s', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def read_lead_i_long_dat_file(dat_file_path, sampling_rate=500, data_format='16', scale_factor=0.001):\n",
    "    \"\"\"\n",
    "    Read a 30-second pure Lead I .dat file directly and properly scale it\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dat_file_path : str\n",
    "        Path to the .dat file (with or without .dat extension)\n",
    "    sampling_rate : int\n",
    "        Sampling rate in Hz (default 500Hz)\n",
    "    data_format : str\n",
    "        Data format of the binary file: '16' for 16-bit integers, '32' for 32-bit floats\n",
    "    scale_factor : float\n",
    "        Scale factor to convert units (0.001 for converting µV to mV)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        ECG signal data for Lead I with shape (total_samples,)\n",
    "    \"\"\"\n",
    "    # Ensure the path ends with .dat\n",
    "    if not dat_file_path.endswith('.dat'):\n",
    "        dat_file_path += '.dat'\n",
    "    \n",
    "    # Expected samples for full 30 seconds\n",
    "    expected_samples = sampling_rate * 30\n",
    "    \n",
    "    # Read the binary data\n",
    "    try:\n",
    "        if data_format == '16':\n",
    "            # 16-bit signed integers (common format for ECG)\n",
    "            data = np.fromfile(dat_file_path, dtype=np.int16)\n",
    "        elif data_format == '32':\n",
    "            # 32-bit floating point (less common)\n",
    "            data = np.fromfile(dat_file_path, dtype=np.float32)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported data format: {data_format}\")\n",
    "    \n",
    "        # Apply scaling to convert µV to mV\n",
    "        signal = data * scale_factor\n",
    "        \n",
    "        # Handle if signal is not exactly 30 seconds\n",
    "        if len(signal) < expected_samples:\n",
    "            # Pad with zeros if too short\n",
    "            padded_signal = np.zeros(expected_samples)\n",
    "            padded_signal[:len(signal)] = signal\n",
    "            signal = padded_signal\n",
    "        elif len(signal) > expected_samples:\n",
    "            # Truncate if too long\n",
    "            signal = signal[:expected_samples]\n",
    "        \n",
    "        return signal\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "def segment_signal(signal, sampling_rate=500):\n",
    "    \"\"\"\n",
    "    Segment a 30-second signal into three 10-second segments\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    signal : numpy.ndarray\n",
    "        The full signal to segment\n",
    "    sampling_rate : int\n",
    "        Sampling rate in Hz\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of three 10-second signal segments\n",
    "    \"\"\"\n",
    "    # Calculate samples per segment (10 seconds)\n",
    "    segment_samples = sampling_rate * 10\n",
    "    \n",
    "    # Expected samples for full 30 seconds\n",
    "    expected_samples = sampling_rate * 30\n",
    "    \n",
    "    # Ensure the signal is 30 seconds long\n",
    "    if len(signal) != expected_samples:\n",
    "        # Resample to 30 seconds\n",
    "        x = np.linspace(0, 1, len(signal))\n",
    "        x_new = np.linspace(0, 1, expected_samples)\n",
    "        f = interp1d(x, signal, kind='linear', bounds_error=False, fill_value=\"extrapolate\")\n",
    "        signal = f(x_new)\n",
    "    \n",
    "    # Split the signal into three 10-second segments\n",
    "    segments = []\n",
    "    for i in range(3):\n",
    "        start_idx = i * segment_samples\n",
    "        end_idx = (i + 1) * segment_samples\n",
    "        segment = signal[start_idx:end_idx]\n",
    "        segments.append(segment)\n",
    "        \n",
    "    return segments\n",
    "\n",
    "def process_segment(segment, sampling_rate=500):\n",
    "    \"\"\"\n",
    "    Process a segment of ECG data to ensure it's properly formatted for the model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    segment : numpy.ndarray\n",
    "        Raw ECG segment\n",
    "    sampling_rate : int\n",
    "        Sampling rate of the ECG\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Processed segment ready for model input\n",
    "    \"\"\"\n",
    "    # Ensure correct length (5000 samples for 10 seconds)\n",
    "    if len(segment) != 5000:\n",
    "        x = np.linspace(0, 1, len(segment))\n",
    "        x_new = np.linspace(0, 1, 5000)\n",
    "        f = interp1d(x, segment, kind='linear', bounds_error=False, fill_value=\"extrapolate\")\n",
    "        segment = f(x_new)\n",
    "    \n",
    "    return segment\n",
    "\n",
    "def predict_with_voting(dat_file_path, model_path, mlb_path=None, sampling_rate=500, scale_factor=0.001, debug=False):\n",
    "    \"\"\"\n",
    "    Process a 30-second .dat file, properly scale it, split it into three 10-second segments,\n",
    "    make predictions on each segment, and return the class with highest average probability.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dat_file_path : str\n",
    "        Path to the .dat file\n",
    "    model_path : str\n",
    "        Path to the saved model (.h5 file)\n",
    "    mlb_path : str, optional\n",
    "        Path to the saved MultiLabelBinarizer pickle file for label decoding\n",
    "    sampling_rate : int\n",
    "        Sampling rate in Hz (default 500Hz)\n",
    "    scale_factor : float\n",
    "        Scale factor to convert units (0.001 for converting µV to mV)\n",
    "    debug : bool\n",
    "        Whether to print debug information\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing segment predictions and final class probabilities\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Read the 30-second ECG data (pure Lead I) and apply scaling\n",
    "        if debug:\n",
    "            print(f\"Reading signal from {dat_file_path}\")\n",
    "        \n",
    "        full_signal = read_lead_i_long_dat_file(\n",
    "            dat_file_path, \n",
    "            sampling_rate=sampling_rate,\n",
    "            scale_factor=scale_factor\n",
    "        )\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Signal loaded: {len(full_signal)} samples, range: {np.min(full_signal):.2f} to {np.max(full_signal):.2f} mV\")\n",
    "        \n",
    "        # Step 2: Split into three 10-second segments\n",
    "        segments = segment_signal(full_signal, sampling_rate)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Split into {len(segments)} segments of {len(segments[0])} samples each\")\n",
    "        \n",
    "        # Step 3: Load the model (load once to improve performance)\n",
    "        if debug:\n",
    "            print(f\"Loading model from {model_path}\")\n",
    "        \n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        # Load MLB if provided\n",
    "        mlb = None\n",
    "        if mlb_path and os.path.exists(mlb_path):\n",
    "            if debug:\n",
    "                print(f\"Loading label binarizer from {mlb_path}\")\n",
    "            with open(mlb_path, 'rb') as f:\n",
    "                mlb = pickle.load(f)\n",
    "        \n",
    "        # Step 4: Process each segment and collect predictions\n",
    "        segment_results = []\n",
    "        all_predictions = []\n",
    "        \n",
    "        for i, segment in enumerate(segments):\n",
    "            if debug:\n",
    "                print(f\"Processing segment {i+1}...\")\n",
    "            \n",
    "            # Process the segment to ensure it's properly formatted\n",
    "            processed_segment = process_segment(segment)\n",
    "            \n",
    "            # Reshape for model input (batch, time, channels)\n",
    "            X = processed_segment.reshape(1, 5000, 1)\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = model.predict(X, verbose=0)\n",
    "            all_predictions.append(predictions[0])\n",
    "            \n",
    "            # Process segment results\n",
    "            segment_result = {\"raw_predictions\": predictions[0].tolist()}\n",
    "            \n",
    "            # Decode labels if MLB is provided\n",
    "            if mlb is not None:\n",
    "                # Add class probabilities\n",
    "                class_probs = {}\n",
    "                for j, class_name in enumerate(mlb.classes_):\n",
    "                    class_probs[class_name] = float(predictions[0][j])\n",
    "                \n",
    "                segment_result[\"class_probabilities\"] = class_probs\n",
    "            \n",
    "            segment_results.append(segment_result)\n",
    "        \n",
    "        # Step 5: Calculate average probabilities across all segments\n",
    "        final_result = {\"segment_results\": segment_results}\n",
    "        \n",
    "        # Average the raw predictions\n",
    "        avg_predictions = np.mean(all_predictions, axis=0)\n",
    "        final_result[\"averaged_raw_predictions\"] = avg_predictions.tolist()\n",
    "        \n",
    "        # Calculate final class probabilities (average across segments)\n",
    "        if mlb is not None:\n",
    "            # Calculate average probability for each class\n",
    "            final_class_probs = {}\n",
    "            for cls_idx, cls_name in enumerate(mlb.classes_):\n",
    "                final_class_probs[cls_name] = float(np.mean([pred[cls_idx] for pred in all_predictions]))\n",
    "            \n",
    "            # Find the class with highest average probability\n",
    "            top_class = max(final_class_probs.items(), key=lambda x: x[1])\n",
    "            top_class_name = top_class[0]\n",
    "            \n",
    "            final_result[\"final_class_probabilities\"] = final_class_probs\n",
    "            final_result[\"top_class\"] = top_class_name\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"Top class by average probability: {top_class_name} ({top_class[1]:.2f})\")\n",
    "        \n",
    "        return final_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error in predict_with_voting: {str(e)}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def analyze_ecg_pdf(pdf_path, model_path, mlb_path=None, temp_dat_file='calibrated_ecg.dat', cleanup=True, debug=False, visualize=False):\n",
    "    \"\"\"\n",
    "    Complete ECG analysis pipeline: digitizes a PDF ECG, analyzes it with the model,\n",
    "    and returns the diagnosis with highest probability.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the ECG PDF file\n",
    "        model_path (str): Path to the model (.h5) file\n",
    "        mlb_path (str, optional): Path to the MultiLabelBinarizer file\n",
    "        temp_dat_file (str, optional): Path to save the temporary digitized file\n",
    "        cleanup (bool, optional): Whether to remove temporary files after processing\n",
    "        debug (bool, optional): Whether to print debug information\n",
    "        visualize (bool, optional): Whether to visualize the digitized signal\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"diagnosis\": str,  # Top diagnosis (highest average probability)\n",
    "            \"probability\": float,  # Probability of top diagnosis\n",
    "            \"all_probabilities\": dict,  # All diagnoses with probabilities\n",
    "            \"digitized_file\": str  # Path to digitized file (if cleanup=False)\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Silence TensorFlow warnings\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Starting ECG analysis pipeline for {pdf_path}\")\n",
    "    \n",
    "    # 1. Digitize ECG from PDF to DAT file\n",
    "    dat_file_path = digitize_ecg_from_pdf(pdf_path, output_file=temp_dat_file, debug=debug)\n",
    "    \n",
    "    # Visualize the digitized signal if requested\n",
    "    if visualize:\n",
    "        signal = read_lead_i_long_dat_file(dat_file_path, scale_factor=0.001)\n",
    "        visualize_ecg_signal(signal, title=f\"Digitized ECG from {os.path.basename(pdf_path)}\")\n",
    "    \n",
    "    # 2. Process DAT file with model\n",
    "    if debug:\n",
    "        print(\"Processing digitized signal with model...\")\n",
    "    \n",
    "    results = predict_with_voting(\n",
    "        dat_file_path,\n",
    "        model_path,\n",
    "        mlb_path,\n",
    "        scale_factor=0.001,  # Convert microvolts to millivolts\n",
    "        debug=debug\n",
    "    )\n",
    "    \n",
    "    # 3. Extract top diagnosis (highest probability)\n",
    "    top_diagnosis = {\n",
    "        \"diagnosis\": None,\n",
    "        \"probability\": 0.0,\n",
    "        \"all_probabilities\": {},\n",
    "        \"digitized_file\": dat_file_path\n",
    "    }\n",
    "    \n",
    "    # If we have class probabilities, find the highest one\n",
    "    if \"final_class_probabilities\" in results:\n",
    "        probs = results[\"final_class_probabilities\"]\n",
    "        top_diagnosis[\"all_probabilities\"] = probs\n",
    "        \n",
    "        # Use the top class directly from the results\n",
    "        if \"top_class\" in results:\n",
    "            top_diagnosis[\"diagnosis\"] = results[\"top_class\"]\n",
    "            top_diagnosis[\"probability\"] = probs[results[\"top_class\"]]\n",
    "            \n",
    "    # Clean up temporary files if requested\n",
    "    if cleanup and os.path.exists(dat_file_path):\n",
    "        if debug:\n",
    "            print(f\"Cleaning up temporary file: {dat_file_path}\")\n",
    "        os.remove(dat_file_path)\n",
    "        top_diagnosis.pop(\"digitized_file\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Analysis complete. Diagnosis: {top_diagnosis['diagnosis']} (Probability: {top_diagnosis['probability']:.2f})\")\n",
    "    \n",
    "    return top_diagnosis\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Path configuration\n",
    "    sample_pdf = 'sample.pdf'\n",
    "    model_path = 'deep-multiclass.h5'  # Update with actual path\n",
    "    mlb_path = 'deep-multiclass.pkl'     # Update with actual path\n",
    "    \n",
    "    # Analyze ECG with debug output and visualization\n",
    "    result = analyze_ecg_pdf(\n",
    "        sample_pdf, \n",
    "        model_path, \n",
    "        mlb_path, \n",
    "        cleanup=False,  # Keep the digitized file\n",
    "        debug=False,     # Print debug information\n",
    "        visualize=False  # Visualize the digitized signal\n",
    "    )\n",
    "    \n",
    "    # Display result\n",
    "    if result[\"diagnosis\"]:\n",
    "        print(f\"Diagnosis: {result['diagnosis']} \")\n",
    "\n",
    "    else:\n",
    "        print(\"No clear diagnosis found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
