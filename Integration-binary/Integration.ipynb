{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41596219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning initial signal parameters...\n",
      "Failed to find 8 beats during learning.\n",
      "Initializing using default parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "Learning initial signal parameters...\n",
      "Failed to find 8 beats during learning.\n",
      "Initializing using default parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "Learning initial signal parameters...\n",
      "Failed to find 8 beats during learning.\n",
      "Initializing using default parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "Classification result: Normal\n"
     ]
    }
   ],
   "source": [
    "import wfdb                          # To read the ECG files\n",
    "from wfdb import processing          # For QRS detection\n",
    "import numpy as np                   # Numerical operations\n",
    "import joblib                        # To load the saved model\n",
    "import pywt                          # For wavelet feature extraction\n",
    "import os                            # For file operations\n",
    "import cv2                           # For image processing\n",
    "from pdf2image import convert_from_path  # For PDF to image conversion\n",
    "import warnings\n",
    "import pickle\n",
    "import sklearn\n",
    "\n",
    "def digitize_ecg_from_pdf(pdf_path, output_file='calibrated_ecg.dat', save_segments=True):\n",
    "    \"\"\"\n",
    "    Process an ECG PDF file and convert it to a .dat signal file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the ECG PDF file\n",
    "        output_file (str): Path to save the output .dat file (default: 'calibrated_ecg.dat')\n",
    "        save_segments (bool): Whether to save individual segments\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (path to the created .dat file, list of paths to segment files)\n",
    "    \"\"\"\n",
    "    # Convert PDF to image\n",
    "    images = convert_from_path(pdf_path)\n",
    "    temp_image_path = 'temp_ecg_image.jpg'\n",
    "    images[0].save(temp_image_path, 'JPEG')\n",
    "    \n",
    "    # Load the image\n",
    "    img = cv2.imread(temp_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    height, width = img.shape\n",
    "    \n",
    "    # Fixed calibration parameters\n",
    "    calibration = {\n",
    "        'seconds_per_pixel': 2.0 / 197.0,  # 197 pixels = 2 seconds\n",
    "        'mv_per_pixel': 1.0 / 78.8,        # 78.8 pixels = 1 mV\n",
    "    }\n",
    "    \n",
    "    # Calculate layer boundaries using percentages\n",
    "    layer1_start = int(height * 35.35 / 100)\n",
    "    layer1_end = int(height * 51.76 / 100)\n",
    "    layer2_start = int(height * 51.82 / 100)\n",
    "    layer2_end = int(height * 69.41 / 100)\n",
    "    layer3_start = int(height * 69.47 / 100)\n",
    "    layer3_end = int(height * 87.06 / 100)\n",
    "    \n",
    "    # Crop each layer\n",
    "    layers = [\n",
    "        img[layer1_start:layer1_end, :],  # Layer 1\n",
    "        img[layer2_start:layer2_end, :],  # Layer 2\n",
    "        img[layer3_start:layer3_end, :]   # Layer 3\n",
    "    ]\n",
    "    \n",
    "    # Process each layer to extract waveform contours\n",
    "    signals = []\n",
    "    time_points = []\n",
    "    layer_duration = 10.0  # Each layer is 10 seconds long\n",
    "    \n",
    "    for i, layer in enumerate(layers):\n",
    "        # Binary thresholding\n",
    "        _, binary = cv2.threshold(layer, 200, 255, cv2.THRESH_BINARY_INV)\n",
    "        \n",
    "        # Detect contours\n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        waveform_contour = max(contours, key=cv2.contourArea)  # Largest contour is the ECG\n",
    "        \n",
    "        # Sort contour points and extract coordinates\n",
    "        sorted_contour = sorted(waveform_contour, key=lambda p: p[0][0])\n",
    "        x_coords = np.array([point[0][0] for point in sorted_contour])\n",
    "        y_coords = np.array([point[0][1] for point in sorted_contour])\n",
    "        \n",
    "        # Calculate isoelectric line (one-third from the bottom)\n",
    "        isoelectric_line_y = layer.shape[0] * 0.6\n",
    "        \n",
    "        # Convert to time using fixed layer duration\n",
    "        x_min, x_max = np.min(x_coords), np.max(x_coords)\n",
    "        time = (x_coords - x_min) / (x_max - x_min) * layer_duration\n",
    "        \n",
    "        # Calculate signal in millivolts and apply baseline correction\n",
    "        signal_mv = (isoelectric_line_y - y_coords) * calibration['mv_per_pixel']\n",
    "        signal_mv = signal_mv - np.mean(signal_mv)\n",
    "        \n",
    "        # Store the time points and calibrated signal\n",
    "        time_points.append(time)\n",
    "        signals.append(signal_mv)\n",
    "    \n",
    "    # Save individual segments if requested\n",
    "    segment_files = []\n",
    "    sampling_frequency = 500  # Standard ECG frequency\n",
    "    samples_per_segment = int(layer_duration * sampling_frequency)  # 5000 samples per 10-second segment\n",
    "    \n",
    "    if save_segments:\n",
    "        base_name = os.path.splitext(output_file)[0]\n",
    "        \n",
    "        for i, signal in enumerate(signals):\n",
    "            # Interpolate to get evenly sampled signal\n",
    "            segment_time = np.linspace(0, layer_duration, samples_per_segment)\n",
    "            interpolated_signal = np.interp(segment_time, time_points[i], signals[i])\n",
    "            \n",
    "            # Normalize and scale\n",
    "            interpolated_signal = interpolated_signal - np.mean(interpolated_signal)\n",
    "            signal_peak = np.max(np.abs(interpolated_signal))\n",
    "            \n",
    "            if signal_peak > 0 and (signal_peak < 0.5 or signal_peak > 4.0):\n",
    "                scaling_factor = 2.0 / signal_peak  # Target peak amplitude of 2.0 mV\n",
    "                interpolated_signal = interpolated_signal * scaling_factor\n",
    "            \n",
    "            # Convert to 16-bit integers\n",
    "            adc_gain = 1000.0\n",
    "            int_signal = (interpolated_signal * adc_gain).astype(np.int16)\n",
    "            \n",
    "            # Save segment\n",
    "            segment_file = f\"{base_name}_segment{i+1}.dat\"\n",
    "            int_signal.reshape(-1, 1).tofile(segment_file)\n",
    "            segment_files.append(segment_file)\n",
    "    \n",
    "    # Combine signals with proper time alignment for the full record\n",
    "    total_duration = layer_duration * len(layers)\n",
    "    num_samples = int(total_duration * sampling_frequency)\n",
    "    combined_time = np.linspace(0, total_duration, num_samples)\n",
    "    combined_signal = np.zeros(num_samples)\n",
    "    \n",
    "    # Place each lead at the correct time position\n",
    "    for i, (time, signal) in enumerate(zip(time_points, signals)):\n",
    "        start_time = i * layer_duration\n",
    "        mask = (combined_time >= start_time) & (combined_time < start_time + layer_duration)\n",
    "        relevant_times = combined_time[mask]\n",
    "        interpolated_signal = np.interp(relevant_times, start_time + time, signal)\n",
    "        combined_signal[mask] = interpolated_signal\n",
    "    \n",
    "    # Baseline correction and amplitude scaling\n",
    "    combined_signal = combined_signal - np.mean(combined_signal)\n",
    "    signal_peak = np.max(np.abs(combined_signal))\n",
    "    target_amplitude = 2.0  # Target peak amplitude in mV\n",
    "    \n",
    "    if signal_peak > 0 and (signal_peak < 0.5 or signal_peak > 4.0):\n",
    "        scaling_factor = target_amplitude / signal_peak\n",
    "        combined_signal = combined_signal * scaling_factor\n",
    "    \n",
    "    # Convert to 16-bit integers and save as .dat file\n",
    "    adc_gain = 1000.0  # Standard gain: 1000 units per mV\n",
    "    int_signal = (combined_signal * adc_gain).astype(np.int16)\n",
    "    int_signal.tofile(output_file)\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    if os.path.exists(temp_image_path):\n",
    "        os.remove(temp_image_path)\n",
    "    \n",
    "    return output_file, segment_files\n",
    "\n",
    "def split_dat_into_segments(file_path, segment_duration=10.0):\n",
    "    \"\"\"\n",
    "    Split a DAT file into equal segments.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the DAT file (without extension)\n",
    "        segment_duration (float): Duration of each segment in seconds\n",
    "        \n",
    "    Returns:\n",
    "        list: Paths to the segment files\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the signal\n",
    "        signal_all_leads, fs = load_dat_signal(file_path)\n",
    "        \n",
    "        # Choose a lead\n",
    "        if signal_all_leads.shape[1] == 1:\n",
    "            lead_index = 0\n",
    "        else:\n",
    "            lead_priority = [1, 0]  # Try Lead II (index 1), then I (index 0)\n",
    "            lead_index = next((i for i in lead_priority if i < signal_all_leads.shape[1]), 0)\n",
    "            \n",
    "        signal = signal_all_leads[:, lead_index]\n",
    "        \n",
    "        # Calculate samples per segment\n",
    "        samples_per_segment = int(segment_duration * fs)\n",
    "        total_samples = len(signal)\n",
    "        num_segments = total_samples // samples_per_segment\n",
    "        \n",
    "        segment_files = []\n",
    "        \n",
    "        # Split and save each segment\n",
    "        base_name = os.path.splitext(file_path)[0]\n",
    "        \n",
    "        for i in range(num_segments):\n",
    "            start_idx = i * samples_per_segment\n",
    "            end_idx = (i + 1) * samples_per_segment\n",
    "            segment = signal[start_idx:end_idx]\n",
    "            \n",
    "            # Save segment\n",
    "            segment_file = f\"{base_name}_segment{i+1}.dat\"\n",
    "            segment.reshape(-1, 1).tofile(segment_file)\n",
    "            segment_files.append(segment_file)\n",
    "                \n",
    "        return segment_files\n",
    "        \n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def load_dat_signal(file_path, n_leads=12, n_samples=5000, dtype=np.int16):\n",
    "    \"\"\"\n",
    "    Load a DAT file containing ECG signal data.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the DAT file (without extension)\n",
    "        n_leads (int): Number of leads in the signal\n",
    "        n_samples (int): Number of samples per lead\n",
    "        dtype: Data type of the signal\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (numpy array of signal data, sampling frequency)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle both cases: with and without .dat extension\n",
    "        if file_path.endswith('.dat'):\n",
    "            dat_path = file_path\n",
    "        else:\n",
    "            dat_path = file_path + '.dat'\n",
    "            \n",
    "        raw = np.fromfile(dat_path, dtype=dtype)\n",
    "        \n",
    "        # Try to infer number of leads if read size doesn't match expected\n",
    "        if raw.size != n_leads * n_samples:\n",
    "            # Check if single lead\n",
    "            if raw.size == n_samples:\n",
    "                signal = raw.reshape(n_samples, 1)\n",
    "                return signal, 500\n",
    "                \n",
    "            # Try common lead counts\n",
    "            possible_leads = [1, 2, 3, 6, 12]\n",
    "            for possible_lead_count in possible_leads:\n",
    "                if raw.size % possible_lead_count == 0:\n",
    "                    actual_samples = raw.size // possible_lead_count\n",
    "                    signal = raw.reshape(actual_samples, possible_lead_count)\n",
    "                    return signal, 500\n",
    "            \n",
    "            # If we can't determine it reliably, reshape as single lead\n",
    "            signal = raw.reshape(-1, 1)\n",
    "            return signal, 500\n",
    "            \n",
    "        # Normal case when size matches expectation\n",
    "        signal = raw.reshape(n_samples, n_leads)\n",
    "        return signal, 500  # Signal + sampling frequency\n",
    "    except Exception as e:\n",
    "        # Return empty signal with single channel\n",
    "        return np.zeros((n_samples, 1)), 500\n",
    "\n",
    "def extract_features_from_signal(signal):\n",
    "    \"\"\"\n",
    "    Extract features from an ECG signal.\n",
    "    \n",
    "    Args:\n",
    "        signal (numpy.ndarray): ECG signal\n",
    "        \n",
    "    Returns:\n",
    "        list: Features extracted from the signal\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    features.append(np.mean(signal))\n",
    "    features.append(np.std(signal))\n",
    "    features.append(np.median(signal))\n",
    "    features.append(np.min(signal))\n",
    "    features.append(np.max(signal))\n",
    "    features.append(np.percentile(signal, 25))\n",
    "    features.append(np.percentile(signal, 75))\n",
    "    features.append(np.mean(np.diff(signal)))\n",
    "        \n",
    "    coeffs = pywt.wavedec(signal, 'db4', level=5)\n",
    "    for i, coeff in enumerate(coeffs):\n",
    "        features.append(np.mean(coeff))\n",
    "        features.append(np.std(coeff))\n",
    "        features.append(np.min(coeff))\n",
    "        features.append(np.max(coeff))\n",
    "        \n",
    "    return features\n",
    "\n",
    "def classify_new_ecg(file_path, model):\n",
    "    \"\"\"\n",
    "    Classify a new ECG file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the ECG file (without extension)\n",
    "        model: The trained model for classification\n",
    "        \n",
    "    Returns:\n",
    "        str: Classification result (\"Normal\", \"Abnormal\", or error message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        signal_all_leads, fs = load_dat_signal(file_path)\n",
    "        \n",
    "        # Choose lead for analysis - priority order\n",
    "        if signal_all_leads.shape[1] == 1:\n",
    "            lead_index = 0\n",
    "        else:\n",
    "            lead_priority = [1, 0]  # Try Lead II (index 1), then I (index 0)\n",
    "            lead_index = next((i for i in lead_priority if i < signal_all_leads.shape[1]), 0)\n",
    "\n",
    "        # Extract the signal\n",
    "        signal = signal_all_leads[:, lead_index]\n",
    "        \n",
    "        # Normalize signal\n",
    "        signal = (signal - np.mean(signal)) / np.std(signal)\n",
    "\n",
    "        # Detect QRS complexes\n",
    "        try:\n",
    "            xqrs = processing.XQRS(sig=signal, fs=fs)\n",
    "            xqrs.detect()\n",
    "            r_peaks = xqrs.qrs_inds\n",
    "        except Exception:\n",
    "            r_peaks = processing.gqrs_detect(sig=signal, fs=fs)\n",
    "\n",
    "        # Check if we found enough QRS complexes\n",
    "        if len(r_peaks) < 5:\n",
    "            return \"Insufficient beats\"\n",
    "\n",
    "        # Calculate RR intervals and QRS durations\n",
    "        rr_intervals = np.diff(r_peaks) / fs\n",
    "        qrs_durations = np.array([r_peaks[i] - r_peaks[i - 1] for i in range(1, len(r_peaks))])\n",
    "\n",
    "        # Extract features\n",
    "        features = extract_features_from_signal(signal)\n",
    "        \n",
    "        # Add rhythm features\n",
    "        features.extend([\n",
    "            len(r_peaks),\n",
    "            np.mean(rr_intervals) if len(rr_intervals) > 0 else 0,\n",
    "            np.std(rr_intervals) if len(rr_intervals) > 0 else 0,\n",
    "            np.median(rr_intervals) if len(rr_intervals) > 0 else 0,\n",
    "            np.mean(qrs_durations) if len(qrs_durations) > 0 else 0,\n",
    "            np.std(qrs_durations) if len(qrs_durations) > 0 else 0\n",
    "        ])\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict([features])[0]\n",
    "        result = \"Abnormal\" if prediction == 1 else \"Normal\"\n",
    "        \n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error: {str(e)}\"\n",
    "        return error_msg\n",
    "\n",
    "def classify_ecg(file_path, model, is_pdf=False):\n",
    "    \"\"\"\n",
    "    Wrapper function that handles both PDF and DAT ECG files with segment voting.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the ECG file (.pdf or without extension for .dat)\n",
    "        model: The trained model for classification\n",
    "        is_pdf (bool): Whether the input file is a PDF (True) or DAT (False)\n",
    "        \n",
    "    Returns:\n",
    "        str: Classification result (\"Normal\", \"Abnormal\", or error message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if model is valid\n",
    "        if model is None:\n",
    "            return \"Error: Model not loaded. Please check model compatibility.\"\n",
    "            \n",
    "        if is_pdf:\n",
    "            # Extract file name without extension for output\n",
    "            base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            output_dat = f\"{base_name}_digitized.dat\"\n",
    "            \n",
    "            # Digitize the PDF to a DAT file and get segment files\n",
    "            dat_path, segment_files = digitize_ecg_from_pdf(\n",
    "                pdf_path=file_path, \n",
    "                output_file=output_dat\n",
    "            )\n",
    "        else:\n",
    "            # For DAT files, we need to split into segments\n",
    "            segment_files = split_dat_into_segments(file_path)\n",
    "            \n",
    "            if not segment_files:\n",
    "                # If splitting failed, try classifying the whole file\n",
    "                return classify_new_ecg(file_path, model)\n",
    "        \n",
    "        # Process each segment and collect votes\n",
    "        segment_results = []\n",
    "        \n",
    "        for i, segment_file in enumerate(segment_files):\n",
    "            # Get file path without extension\n",
    "            segment_path = os.path.splitext(segment_file)[0]\n",
    "            \n",
    "            # Classify this segment\n",
    "            result = classify_new_ecg(segment_path, model)\n",
    "            segment_results.append(result)\n",
    "            \n",
    "            # Remove temporary segment files\n",
    "            try:\n",
    "                os.remove(segment_file)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Count results and use majority voting\n",
    "        if segment_results:\n",
    "            normal_count = segment_results.count(\"Normal\")\n",
    "            abnormal_count = segment_results.count(\"Abnormal\")\n",
    "            \n",
    "            # Decision rules:\n",
    "            # 1. If any segment is abnormal, classify as abnormal\n",
    "            # 2. Only classify as normal if majority of segments are normal\n",
    "            if abnormal_count > normal_count:\n",
    "                final_result = \"Abnormal\"\n",
    "            elif normal_count > abnormal_count:\n",
    "                final_result = \"Normal\"\n",
    "            else:\n",
    "                final_result = \"Inconclusive\"\n",
    "                \n",
    "            return final_result\n",
    "        else:\n",
    "            return \"Error: No valid segments to classify\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Classification error: {str(e)}\"\n",
    "        return error_msg\n",
    "\n",
    "# Load the saved model\n",
    "try:\n",
    "    model_path = 'voting_classifier.pkl'\n",
    "    if os.path.exists(model_path):\n",
    "        voting_loaded = joblib.load(model_path)\n",
    "    else:\n",
    "        # Try to find the model in the current or parent directories\n",
    "        for root, dirs, files in os.walk('.'):\n",
    "            for file in files:\n",
    "                if file.endswith('.pkl') and 'voting' in file.lower():\n",
    "                    model_path = os.path.join(root, file)\n",
    "                    voting_loaded = joblib.load(model_path)\n",
    "                    break\n",
    "            if 'voting_loaded' in locals():\n",
    "                break\n",
    "                \n",
    "        if 'voting_loaded' not in locals():\n",
    "            voting_loaded = None\n",
    "except Exception as e:\n",
    "    voting_loaded = None\n",
    "\n",
    "# Simple test for the classify_ecg function\n",
    "test_pdf_path = \"sample.pdf\"\n",
    "if os.path.exists(test_pdf_path) and voting_loaded is not None:\n",
    "    result_pdf = classify_ecg(test_pdf_path, voting_loaded, is_pdf=True)\n",
    "    print(f\"Classification result: {result_pdf}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
