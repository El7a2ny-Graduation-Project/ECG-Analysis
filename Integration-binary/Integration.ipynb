{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41596219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded classification model from voting_classifier.pkl\n",
      "\n",
      "----- Testing PDF file classification with segment voting -----\n",
      "Found test PDF file: sample.pdf\n",
      "Processing PDF file: sample.pdf\n",
      "Starting ECG digitization from PDF: sample.pdf\n",
      "Converted PDF to image: temp_ecg_image.jpg\n",
      "Image dimensions: 2200x1700\n",
      "Calibration parameters: {'seconds_per_pixel': 0.01015228426395939, 'mv_per_pixel': 0.012690355329949238}\n",
      "Layer 1 boundaries: 600-879\n",
      "Layer 2 boundaries: 880-1179\n",
      "Layer 3 boundaries: 1180-1480\n",
      "Processing layer 1...\n",
      "  - Found 3912 contours\n",
      "  - Selected contour with 3830 points\n",
      "  - Layer 1 signal range: -0.81 mV to 1.49 mV\n",
      "Processing layer 2...\n",
      "  - Found 4864 contours\n",
      "  - Selected contour with 3570 points\n",
      "  - Layer 2 signal range: -0.56 mV to 1.18 mV\n",
      "Processing layer 3...\n",
      "  - Found 4067 contours\n",
      "  - Selected contour with 3917 points\n",
      "  - Layer 3 signal range: -0.49 mV to 1.20 mV\n",
      "Saved segment 1 to sample_digitized_segment1.dat\n",
      "Saved segment 2 to sample_digitized_segment2.dat\n",
      "Saved segment 3 to sample_digitized_segment3.dat\n",
      "Combining signals with 500 Hz sampling rate, total duration: 30.0s\n",
      "  - Added layer 1 signal from 0.0s to 10.0s\n",
      "  - Added layer 2 signal from 10.0s to 20.0s\n",
      "  - Added layer 3 signal from 20.0s to 30.0s\n",
      "Signal peak before scaling: 1.55 mV\n",
      "Saved signal to sample_digitized.dat with 15000 samples\n",
      "Integer signal range: -733 to 1547\n",
      "Removed temporary image: temp_ecg_image.jpg\n",
      "Digitized ECG saved to: sample_digitized.dat\n",
      "Created 3 segment files\n",
      "\n",
      "--- Processing Segment 1 ---\n",
      "Classifying ECG from: sample_digitized_segment1\n",
      "Loading signal from: sample_digitized_segment1.dat\n",
      "Raw data size: 5000\n",
      "Unexpected size: 5000, expected 60000\n",
      "Attempting to infer number of leads...\n",
      "Detected single lead signal\n",
      "Loaded signal with shape (5000, 1), sampling rate 500 Hz\n",
      "Using single lead\n",
      "Signal normalized\n",
      "Detecting QRS complexes...\n",
      "Learning initial signal parameters...\n",
      "Failed to find 8 beats during learning.\n",
      "Initializing using default parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "Detected 13 QRS complexes with XQRS method\n",
      "Mean RR interval: 0.7660 s\n",
      "Mean QRS duration: 0.7660 s\n",
      "Extracting features from signal...\n",
      "Computing wavelet decomposition...\n",
      "Wavelet features for level 0: mean=0.3550, std=5.1168\n",
      "Extracted 32 features\n",
      "Final feature vector length: 38\n",
      "Classification result: Normal (prediction value: 0)\n",
      "Segment 1 classification: Normal\n",
      "Removed temporary segment file: sample_digitized_segment1.dat\n",
      "\n",
      "--- Processing Segment 2 ---\n",
      "Classifying ECG from: sample_digitized_segment2\n",
      "Loading signal from: sample_digitized_segment2.dat\n",
      "Raw data size: 5000\n",
      "Unexpected size: 5000, expected 60000\n",
      "Attempting to infer number of leads...\n",
      "Detected single lead signal\n",
      "Loaded signal with shape (5000, 1), sampling rate 500 Hz\n",
      "Using single lead\n",
      "Signal normalized\n",
      "Detecting QRS complexes...\n",
      "Learning initial signal parameters...\n",
      "Failed to find 8 beats during learning.\n",
      "Initializing using default parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "Detected 13 QRS complexes with XQRS method\n",
      "Mean RR interval: 0.7398 s\n",
      "Mean QRS duration: 0.7398 s\n",
      "Extracting features from signal...\n",
      "Computing wavelet decomposition...\n",
      "Wavelet features for level 0: mean=0.0739, std=4.8274\n",
      "Extracted 32 features\n",
      "Final feature vector length: 38\n",
      "Classification result: Abnormal (prediction value: 1)\n",
      "Segment 2 classification: Abnormal\n",
      "Removed temporary segment file: sample_digitized_segment2.dat\n",
      "\n",
      "--- Processing Segment 3 ---\n",
      "Classifying ECG from: sample_digitized_segment3\n",
      "Loading signal from: sample_digitized_segment3.dat\n",
      "Raw data size: 5000\n",
      "Unexpected size: 5000, expected 60000\n",
      "Attempting to infer number of leads...\n",
      "Detected single lead signal\n",
      "Loaded signal with shape (5000, 1), sampling rate 500 Hz\n",
      "Using single lead\n",
      "Signal normalized\n",
      "Detecting QRS complexes...\n",
      "Learning initial signal parameters...\n",
      "Failed to find 8 beats during learning.\n",
      "Initializing using default parameters\n",
      "Running QRS detection...\n",
      "QRS detection complete.\n",
      "Detected 13 QRS complexes with XQRS method\n",
      "Mean RR interval: 0.7132 s\n",
      "Mean QRS duration: 0.7132 s\n",
      "Extracting features from signal...\n",
      "Computing wavelet decomposition...\n",
      "Wavelet features for level 0: mean=-0.1348, std=4.7111\n",
      "Extracted 32 features\n",
      "Final feature vector length: 38\n",
      "Classification result: Normal (prediction value: 0)\n",
      "Segment 3 classification: Normal\n",
      "Removed temporary segment file: sample_digitized_segment3.dat\n",
      "\n",
      "--- Voting Results ---\n",
      "Normal votes: 2\n",
      "Abnormal votes: 1\n",
      "Errors/Inconclusive: 0\n",
      "Final decision: Normal\n",
      "PDF file classification result (segment voting): Normal\n"
     ]
    }
   ],
   "source": [
    "import wfdb                          # To read the ECG files\n",
    "from wfdb import processing          # For QRS detection\n",
    "import numpy as np                   # Numerical operations\n",
    "import joblib                        # To load the saved model\n",
    "import pywt                          # For wavelet feature extraction\n",
    "import os                            # For file operations\n",
    "import cv2                           # For image processing\n",
    "from pdf2image import convert_from_path  # For PDF to image conversion\n",
    "import warnings\n",
    "import pickle\n",
    "import sklearn\n",
    "\n",
    "# Let's modify the digitize_ecg_from_pdf function to return segment information\n",
    "def digitize_ecg_from_pdf(pdf_path, output_file='calibrated_ecg.dat', debug=False, save_segments=True):\n",
    "    \"\"\"\n",
    "    Process an ECG PDF file and convert it to a .dat signal file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the ECG PDF file\n",
    "        output_file (str): Path to save the output .dat file (default: 'calibrated_ecg.dat')\n",
    "        debug (bool): Whether to print debug information\n",
    "        save_segments (bool): Whether to save individual segments\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (path to the created .dat file, list of paths to segment files)\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"Starting ECG digitization from PDF: {pdf_path}\")\n",
    "    \n",
    "    # Convert PDF to image\n",
    "    images = convert_from_path(pdf_path)\n",
    "    temp_image_path = 'temp_ecg_image.jpg'\n",
    "    images[0].save(temp_image_path, 'JPEG')\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Converted PDF to image: {temp_image_path}\")\n",
    "    \n",
    "    # Load the image\n",
    "    img = cv2.imread(temp_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    height, width = img.shape\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Image dimensions: {width}x{height}\")\n",
    "    \n",
    "    # Fixed calibration parameters\n",
    "    calibration = {\n",
    "        'seconds_per_pixel': 2.0 / 197.0,  # 197 pixels = 2 seconds\n",
    "        'mv_per_pixel': 1.0 / 78.8,        # 78.8 pixels = 1 mV\n",
    "    }\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Calibration parameters: {calibration}\")\n",
    "    \n",
    "    # Calculate layer boundaries using percentages\n",
    "    layer1_start = int(height * 35.35 / 100)\n",
    "    layer1_end = int(height * 51.76 / 100)\n",
    "    layer2_start = int(height * 51.82 / 100)\n",
    "    layer2_end = int(height * 69.41 / 100)\n",
    "    layer3_start = int(height * 69.47 / 100)\n",
    "    layer3_end = int(height * 87.06 / 100)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Layer 1 boundaries: {layer1_start}-{layer1_end}\")\n",
    "        print(f\"Layer 2 boundaries: {layer2_start}-{layer2_end}\")\n",
    "        print(f\"Layer 3 boundaries: {layer3_start}-{layer3_end}\")\n",
    "    \n",
    "    # Crop each layer\n",
    "    layers = [\n",
    "        img[layer1_start:layer1_end, :],  # Layer 1\n",
    "        img[layer2_start:layer2_end, :],  # Layer 2\n",
    "        img[layer3_start:layer3_end, :]   # Layer 3\n",
    "    ]\n",
    "    \n",
    "    # Process each layer to extract waveform contours\n",
    "    signals = []\n",
    "    time_points = []\n",
    "    layer_duration = 10.0  # Each layer is 10 seconds long\n",
    "    \n",
    "    for i, layer in enumerate(layers):\n",
    "        if debug:\n",
    "            print(f\"Processing layer {i+1}...\")\n",
    "        \n",
    "        # Binary thresholding\n",
    "        _, binary = cv2.threshold(layer, 200, 255, cv2.THRESH_BINARY_INV)\n",
    "        \n",
    "        # Detect contours\n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        waveform_contour = max(contours, key=cv2.contourArea)  # Largest contour is the ECG\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"  - Found {len(contours)} contours\")\n",
    "            print(f\"  - Selected contour with {len(waveform_contour)} points\")\n",
    "        \n",
    "        # Sort contour points and extract coordinates\n",
    "        sorted_contour = sorted(waveform_contour, key=lambda p: p[0][0])\n",
    "        x_coords = np.array([point[0][0] for point in sorted_contour])\n",
    "        y_coords = np.array([point[0][1] for point in sorted_contour])\n",
    "        \n",
    "        # Calculate isoelectric line (one-third from the bottom)\n",
    "        isoelectric_line_y = layer.shape[0] * 0.6\n",
    "        \n",
    "        # Convert to time using fixed layer duration\n",
    "        x_min, x_max = np.min(x_coords), np.max(x_coords)\n",
    "        time = (x_coords - x_min) / (x_max - x_min) * layer_duration\n",
    "        \n",
    "        # Calculate signal in millivolts and apply baseline correction\n",
    "        signal_mv = (isoelectric_line_y - y_coords) * calibration['mv_per_pixel']\n",
    "        signal_mv = signal_mv - np.mean(signal_mv)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"  - Layer {i+1} signal range: {np.min(signal_mv):.2f} mV to {np.max(signal_mv):.2f} mV\")\n",
    "        \n",
    "        # Store the time points and calibrated signal\n",
    "        time_points.append(time)\n",
    "        signals.append(signal_mv)\n",
    "    \n",
    "    # Save individual segments if requested\n",
    "    segment_files = []\n",
    "    sampling_frequency = 500  # Standard ECG frequency\n",
    "    samples_per_segment = int(layer_duration * sampling_frequency)  # 5000 samples per 10-second segment\n",
    "    \n",
    "    if save_segments:\n",
    "        base_name = os.path.splitext(output_file)[0]\n",
    "        \n",
    "        for i, signal in enumerate(signals):\n",
    "            # Interpolate to get evenly sampled signal\n",
    "            segment_time = np.linspace(0, layer_duration, samples_per_segment)\n",
    "            interpolated_signal = np.interp(segment_time, time_points[i], signals[i])\n",
    "            \n",
    "            # Normalize and scale\n",
    "            interpolated_signal = interpolated_signal - np.mean(interpolated_signal)\n",
    "            signal_peak = np.max(np.abs(interpolated_signal))\n",
    "            \n",
    "            if signal_peak > 0 and (signal_peak < 0.5 or signal_peak > 4.0):\n",
    "                scaling_factor = 2.0 / signal_peak  # Target peak amplitude of 2.0 mV\n",
    "                interpolated_signal = interpolated_signal * scaling_factor\n",
    "            \n",
    "            # Convert to 16-bit integers\n",
    "            adc_gain = 1000.0\n",
    "            int_signal = (interpolated_signal * adc_gain).astype(np.int16)\n",
    "            \n",
    "            # Save segment\n",
    "            segment_file = f\"{base_name}_segment{i+1}.dat\"\n",
    "            int_signal.reshape(-1, 1).tofile(segment_file)\n",
    "            segment_files.append(segment_file)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"Saved segment {i+1} to {segment_file}\")\n",
    "    \n",
    "    # Combine signals with proper time alignment for the full record\n",
    "    total_duration = layer_duration * len(layers)\n",
    "    num_samples = int(total_duration * sampling_frequency)\n",
    "    combined_time = np.linspace(0, total_duration, num_samples)\n",
    "    combined_signal = np.zeros(num_samples)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Combining signals with {sampling_frequency} Hz sampling rate, total duration: {total_duration}s\")\n",
    "    \n",
    "    # Place each lead at the correct time position\n",
    "    for i, (time, signal) in enumerate(zip(time_points, signals)):\n",
    "        start_time = i * layer_duration\n",
    "        mask = (combined_time >= start_time) & (combined_time < start_time + layer_duration)\n",
    "        relevant_times = combined_time[mask]\n",
    "        interpolated_signal = np.interp(relevant_times, start_time + time, signal)\n",
    "        combined_signal[mask] = interpolated_signal\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"  - Added layer {i+1} signal from {start_time}s to {start_time + layer_duration}s\")\n",
    "    \n",
    "    # Baseline correction and amplitude scaling\n",
    "    combined_signal = combined_signal - np.mean(combined_signal)\n",
    "    signal_peak = np.max(np.abs(combined_signal))\n",
    "    target_amplitude = 2.0  # Target peak amplitude in mV\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Signal peak before scaling: {signal_peak:.2f} mV\")\n",
    "    \n",
    "    if signal_peak > 0 and (signal_peak < 0.5 or signal_peak > 4.0):\n",
    "        scaling_factor = target_amplitude / signal_peak\n",
    "        combined_signal = combined_signal * scaling_factor\n",
    "        if debug:\n",
    "            print(f\"Applied scaling factor: {scaling_factor:.2f}\")\n",
    "            print(f\"Signal peak after scaling: {np.max(np.abs(combined_signal)):.2f} mV\")\n",
    "    \n",
    "    # Convert to 16-bit integers and save as .dat file\n",
    "    adc_gain = 1000.0  # Standard gain: 1000 units per mV\n",
    "    int_signal = (combined_signal * adc_gain).astype(np.int16)\n",
    "    int_signal.tofile(output_file)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Saved signal to {output_file} with {len(int_signal)} samples\")\n",
    "        print(f\"Integer signal range: {np.min(int_signal)} to {np.max(int_signal)}\")\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    if os.path.exists(temp_image_path):\n",
    "        os.remove(temp_image_path)\n",
    "        if debug:\n",
    "            print(f\"Removed temporary image: {temp_image_path}\")\n",
    "    \n",
    "    return output_file, segment_files\n",
    "\n",
    "# Add a function to split a DAT file into segments\n",
    "def split_dat_into_segments(file_path, segment_duration=10.0, debug=False):\n",
    "    \"\"\"\n",
    "    Split a DAT file into equal segments.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the DAT file (without extension)\n",
    "        segment_duration (float): Duration of each segment in seconds\n",
    "        debug (bool): Whether to print debug information\n",
    "        \n",
    "    Returns:\n",
    "        list: Paths to the segment files\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the signal\n",
    "        signal_all_leads, fs = load_dat_signal(file_path, debug=debug)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Loaded signal with shape {signal_all_leads.shape}\")\n",
    "        \n",
    "        # Choose a lead\n",
    "        if signal_all_leads.shape[1] == 1:\n",
    "            lead_index = 0\n",
    "        else:\n",
    "            lead_priority = [1, 0]  # Try Lead II (index 1), then I (index 0)\n",
    "            lead_index = next((i for i in lead_priority if i < signal_all_leads.shape[1]), 0)\n",
    "            \n",
    "        signal = signal_all_leads[:, lead_index]\n",
    "        \n",
    "        # Calculate samples per segment\n",
    "        samples_per_segment = int(segment_duration * fs)\n",
    "        total_samples = len(signal)\n",
    "        num_segments = total_samples // samples_per_segment\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Splitting signal into {num_segments} segments of {segment_duration} seconds each\")\n",
    "            \n",
    "        segment_files = []\n",
    "        \n",
    "        # Split and save each segment\n",
    "        base_name = os.path.splitext(file_path)[0]\n",
    "        \n",
    "        for i in range(num_segments):\n",
    "            start_idx = i * samples_per_segment\n",
    "            end_idx = (i + 1) * samples_per_segment\n",
    "            segment = signal[start_idx:end_idx]\n",
    "            \n",
    "            # Save segment\n",
    "            segment_file = f\"{base_name}_segment{i+1}.dat\"\n",
    "            segment.reshape(-1, 1).tofile(segment_file)\n",
    "            segment_files.append(segment_file)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"Saved segment {i+1} to {segment_file}\")\n",
    "                \n",
    "        return segment_files\n",
    "        \n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error splitting DAT file: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Add function to load DAT signals\n",
    "def load_dat_signal(file_path, n_leads=12, n_samples=5000, dtype=np.int16, debug=False):\n",
    "    \"\"\"\n",
    "    Load a DAT file containing ECG signal data.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the DAT file (without extension)\n",
    "        n_leads (int): Number of leads in the signal\n",
    "        n_samples (int): Number of samples per lead\n",
    "        dtype: Data type of the signal\n",
    "        debug (bool): Whether to print debug information\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (numpy array of signal data, sampling frequency)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle both cases: with and without .dat extension\n",
    "        if file_path.endswith('.dat'):\n",
    "            dat_path = file_path\n",
    "        else:\n",
    "            dat_path = file_path + '.dat'\n",
    "            \n",
    "        if debug:\n",
    "            print(f\"Loading signal from: {dat_path}\")\n",
    "            \n",
    "        raw = np.fromfile(dat_path, dtype=dtype)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Raw data size: {raw.size}\")\n",
    "            \n",
    "        # Try to infer number of leads if read size doesn't match expected\n",
    "        if raw.size != n_leads * n_samples:\n",
    "            if debug:\n",
    "                print(f\"Unexpected size: {raw.size}, expected {n_leads * n_samples}\")\n",
    "                print(\"Attempting to infer number of leads...\")\n",
    "                \n",
    "            # Check if single lead\n",
    "            if raw.size == n_samples:\n",
    "                if debug:\n",
    "                    print(\"Detected single lead signal\")\n",
    "                signal = raw.reshape(n_samples, 1)\n",
    "                return signal, 500\n",
    "                \n",
    "            # Try common lead counts\n",
    "            possible_leads = [1, 2, 3, 6, 12]\n",
    "            for possible_lead_count in possible_leads:\n",
    "                if raw.size % possible_lead_count == 0:\n",
    "                    actual_samples = raw.size // possible_lead_count\n",
    "                    if debug:\n",
    "                        print(f\"Inferred {possible_lead_count} leads with {actual_samples} samples each\")\n",
    "                    signal = raw.reshape(actual_samples, possible_lead_count)\n",
    "                    return signal, 500\n",
    "            \n",
    "            # If we can't determine it reliably, reshape as single lead\n",
    "            if debug:\n",
    "                print(\"Could not infer lead count, reshaping as single lead\")\n",
    "            signal = raw.reshape(-1, 1)\n",
    "            return signal, 500\n",
    "            \n",
    "        # Normal case when size matches expectation\n",
    "        signal = raw.reshape(n_samples, n_leads)\n",
    "        return signal, 500  # Signal + sampling frequency\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error loading DAT file: {str(e)}\")\n",
    "        # Return empty signal with single channel\n",
    "        return np.zeros((n_samples, 1)), 500\n",
    "\n",
    "# Add the feature extraction function\n",
    "def extract_features_from_signal(signal, debug=False):\n",
    "    \"\"\"\n",
    "    Extract features from an ECG signal.\n",
    "    \n",
    "    Args:\n",
    "        signal (numpy.ndarray): ECG signal\n",
    "        debug (bool): Whether to print debug information\n",
    "        \n",
    "    Returns:\n",
    "        list: Features extracted from the signal\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(\"Extracting features from signal...\")\n",
    "        \n",
    "    features = []\n",
    "    features.append(np.mean(signal))\n",
    "    features.append(np.std(signal))\n",
    "    features.append(np.median(signal))\n",
    "    features.append(np.min(signal))\n",
    "    features.append(np.max(signal))\n",
    "    features.append(np.percentile(signal, 25))\n",
    "    features.append(np.percentile(signal, 75))\n",
    "    features.append(np.mean(np.diff(signal)))\n",
    "\n",
    "    if debug:\n",
    "        print(\"Computing wavelet decomposition...\")\n",
    "        \n",
    "    coeffs = pywt.wavedec(signal, 'db4', level=5)\n",
    "    for i, coeff in enumerate(coeffs):\n",
    "        features.append(np.mean(coeff))\n",
    "        features.append(np.std(coeff))\n",
    "        features.append(np.min(coeff))\n",
    "        features.append(np.max(coeff))\n",
    "        \n",
    "        if debug and i == 0:\n",
    "            print(f\"Wavelet features for level {i}: mean={np.mean(coeff):.4f}, std={np.std(coeff):.4f}\")\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Extracted {len(features)} features\")\n",
    "        \n",
    "    return features\n",
    "\n",
    "# Add the classify_new_ecg function\n",
    "def classify_new_ecg(file_path, model, debug=False):\n",
    "    \"\"\"\n",
    "    Classify a new ECG file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the ECG file (without extension)\n",
    "        model: The trained model for classification\n",
    "        debug (bool): Whether to print debug information\n",
    "        \n",
    "    Returns:\n",
    "        str: Classification result (\"Normal\", \"Abnormal\", or error message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if debug:\n",
    "            print(f\"Classifying ECG from: {file_path}\")\n",
    "            \n",
    "        signal_all_leads, fs = load_dat_signal(file_path, debug=debug)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Loaded signal with shape {signal_all_leads.shape}, sampling rate {fs} Hz\")\n",
    "            \n",
    "        # Choose lead for analysis - priority order\n",
    "        if signal_all_leads.shape[1] == 1:\n",
    "            lead_index = 0\n",
    "            if debug:\n",
    "                print(\"Using single lead\")\n",
    "        else:\n",
    "            lead_priority = [1, 0]  # Try Lead II (index 1), then I (index 0)\n",
    "            lead_index = next((i for i in lead_priority if i < signal_all_leads.shape[1]), 0)\n",
    "            if debug:\n",
    "                print(f\"Using lead index {lead_index}\")\n",
    "\n",
    "        # Extract the signal\n",
    "        signal = signal_all_leads[:, lead_index]\n",
    "        \n",
    "        # Normalize signal\n",
    "        signal = (signal - np.mean(signal)) / np.std(signal)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"Signal normalized\")\n",
    "            print(f\"Detecting QRS complexes...\")\n",
    "\n",
    "        # Detect QRS complexes\n",
    "        try:\n",
    "            xqrs = processing.XQRS(sig=signal, fs=fs)\n",
    "            xqrs.detect()\n",
    "            r_peaks = xqrs.qrs_inds\n",
    "            if debug:\n",
    "                print(f\"Detected {len(r_peaks)} QRS complexes with XQRS method\")\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"XQRS detection failed: {str(e)}\")\n",
    "                print(\"Falling back to GQRS detector\")\n",
    "            r_peaks = processing.gqrs_detect(sig=signal, fs=fs)\n",
    "            if debug:\n",
    "                print(f\"Detected {len(r_peaks)} QRS complexes with GQRS method\")\n",
    "\n",
    "        # Check if we found enough QRS complexes\n",
    "        if len(r_peaks) < 5:\n",
    "            if debug:\n",
    "                print(f\"Insufficient beats detected: {len(r_peaks)}\")\n",
    "            return \"Insufficient beats\"\n",
    "\n",
    "        # Calculate RR intervals and QRS durations\n",
    "        rr_intervals = np.diff(r_peaks) / fs\n",
    "        qrs_durations = np.array([r_peaks[i] - r_peaks[i - 1] for i in range(1, len(r_peaks))])\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Mean RR interval: {np.mean(rr_intervals):.4f} s\")\n",
    "            print(f\"Mean QRS duration: {np.mean(qrs_durations) / fs:.4f} s\")\n",
    "\n",
    "        # Extract features\n",
    "        features = extract_features_from_signal(signal, debug=debug)\n",
    "        \n",
    "        # Add rhythm features\n",
    "        features.extend([\n",
    "            len(r_peaks),\n",
    "            np.mean(rr_intervals) if len(rr_intervals) > 0 else 0,\n",
    "            np.std(rr_intervals) if len(rr_intervals) > 0 else 0,\n",
    "            np.median(rr_intervals) if len(rr_intervals) > 0 else 0,\n",
    "            np.mean(qrs_durations) if len(qrs_durations) > 0 else 0,\n",
    "            np.std(qrs_durations) if len(qrs_durations) > 0 else 0\n",
    "        ])\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Final feature vector length: {len(features)}\")\n",
    "            \n",
    "        # Make prediction\n",
    "        prediction = model.predict([features])[0]\n",
    "        result = \"Abnormal\" if prediction == 1 else \"Normal\"\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Classification result: {result} (prediction value: {prediction})\")\n",
    "            \n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error: {str(e)}\"\n",
    "        if debug:\n",
    "            print(error_msg)\n",
    "        return error_msg\n",
    "\n",
    "# Modify the classify_ecg wrapper function to use the voting approach\n",
    "def classify_ecg(file_path, model, is_pdf=False, debug=False):\n",
    "    \"\"\"\n",
    "    Wrapper function that handles both PDF and DAT ECG files with segment voting.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the ECG file (.pdf or without extension for .dat)\n",
    "        model: The trained model for classification\n",
    "        is_pdf (bool): Whether the input file is a PDF (True) or DAT (False)\n",
    "        debug (bool): Enable debug output\n",
    "        \n",
    "    Returns:\n",
    "        str: Classification result (\"Normal\", \"Abnormal\", or error message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if model is valid\n",
    "        if model is None:\n",
    "            return \"Error: Model not loaded. Please check model compatibility.\"\n",
    "            \n",
    "        if is_pdf:\n",
    "            if debug:\n",
    "                print(f\"Processing PDF file: {file_path}\")\n",
    "            \n",
    "            # Extract file name without extension for output\n",
    "            base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            output_dat = f\"{base_name}_digitized.dat\"\n",
    "            \n",
    "            # Digitize the PDF to a DAT file and get segment files\n",
    "            dat_path, segment_files = digitize_ecg_from_pdf(\n",
    "                pdf_path=file_path, \n",
    "                output_file=output_dat, \n",
    "                debug=debug\n",
    "            )\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"Digitized ECG saved to: {dat_path}\")\n",
    "                print(f\"Created {len(segment_files)} segment files\")\n",
    "        else:\n",
    "            if debug:\n",
    "                print(f\"Processing DAT file: {file_path}\")\n",
    "            \n",
    "            # For DAT files, we need to split into segments\n",
    "            segment_files = split_dat_into_segments(file_path, debug=debug)\n",
    "            \n",
    "            if not segment_files:\n",
    "                # If splitting failed, try classifying the whole file\n",
    "                return classify_new_ecg(file_path, model, debug=debug)\n",
    "        \n",
    "        # Process each segment and collect votes\n",
    "        segment_results = []\n",
    "        \n",
    "        for i, segment_file in enumerate(segment_files):\n",
    "            if debug:\n",
    "                print(f\"\\n--- Processing Segment {i+1} ---\")\n",
    "                \n",
    "            # Get file path without extension\n",
    "            segment_path = os.path.splitext(segment_file)[0]\n",
    "            \n",
    "            # Classify this segment\n",
    "            result = classify_new_ecg(segment_path, model, debug=debug)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"Segment {i+1} classification: {result}\")\n",
    "                \n",
    "            segment_results.append(result)\n",
    "            \n",
    "            # Remove temporary segment files\n",
    "            try:\n",
    "                os.remove(segment_file)\n",
    "                if debug:\n",
    "                    print(f\"Removed temporary segment file: {segment_file}\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Count results and use majority voting\n",
    "        if segment_results:\n",
    "            normal_count = segment_results.count(\"Normal\")\n",
    "            abnormal_count = segment_results.count(\"Abnormal\")\n",
    "            error_count = len(segment_results) - normal_count - abnormal_count\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"\\n--- Voting Results ---\")\n",
    "                print(f\"Normal votes: {normal_count}\")\n",
    "                print(f\"Abnormal votes: {abnormal_count}\")\n",
    "                print(f\"Errors/Inconclusive: {error_count}\")\n",
    "            \n",
    "            # Decision rules:\n",
    "            # 1. If any segment is abnormal, classify as abnormal\n",
    "            # 2. Only classify as normal if majority of segments are normal\n",
    "            if abnormal_count > normal_count:\n",
    "                final_result = \"Abnormal\"\n",
    "            elif normal_count > abnormal_count:\n",
    "                final_result = \"Normal\"\n",
    "            else:\n",
    "                final_result = \"Inconclusive\"\n",
    "                \n",
    "            if debug:\n",
    "                print(f\"Final decision: {final_result}\")\n",
    "                \n",
    "            return final_result\n",
    "        else:\n",
    "            return \"Error: No valid segments to classify\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Classification error: {str(e)}\"\n",
    "        if debug:\n",
    "            print(error_msg)\n",
    "        return error_msg\n",
    "\n",
    "# Load the saved model\n",
    "try:\n",
    "    model_path = 'voting_classifier.pkl'\n",
    "    if os.path.exists(model_path):\n",
    "        voting_loaded = joblib.load(model_path)\n",
    "        print(f\"Successfully loaded classification model from {model_path}\")\n",
    "    else:\n",
    "        print(f\"Model file not found: {model_path}\")\n",
    "        print(\"Attempting to locate model file...\")\n",
    "        \n",
    "        # Try to find the model in the current or parent directories\n",
    "        for root, dirs, files in os.walk('.'):\n",
    "            for file in files:\n",
    "                if file.endswith('.pkl') and 'voting' in file.lower():\n",
    "                    model_path = os.path.join(root, file)\n",
    "                    print(f\"Found potential model file: {model_path}\")\n",
    "                    voting_loaded = joblib.load(model_path)\n",
    "                    print(f\"Successfully loaded model from {model_path}\")\n",
    "                    break\n",
    "            if 'voting_loaded' in locals():\n",
    "                break\n",
    "                \n",
    "        if 'voting_loaded' not in locals():\n",
    "            # If we still can't find it, create a dummy model for demonstration\n",
    "            print(\"No model found. Creating a dummy model for demonstration.\")\n",
    "            from sklearn.ensemble import VotingClassifier\n",
    "            from sklearn.tree import DecisionTreeClassifier\n",
    "            \n",
    "            # Create a simple dummy model\n",
    "            dummy_model = DecisionTreeClassifier(max_depth=2)\n",
    "            voting_loaded = VotingClassifier(estimators=[('dt', dummy_model)], voting='soft')\n",
    "            # This won't actually work for prediction but allows the code to run\n",
    "            print(\"WARNING: Using a dummy model that won't provide valid predictions\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {str(e)}\")\n",
    "    print(\"Creating a placeholder model object\")\n",
    "    # Create a placeholder that will show errors when used\n",
    "    class PlaceholderModel:\n",
    "        def predict(self, X):\n",
    "            raise RuntimeError(\"No valid model loaded\")\n",
    "    voting_loaded = PlaceholderModel()\n",
    "\n",
    "print(\"\\n----- Testing PDF file classification with segment voting -----\")\n",
    "try:\n",
    "    # Check if the test PDF file exists\n",
    "    test_pdf_path = \"sample.pdf\"\n",
    "    if os.path.exists(test_pdf_path):\n",
    "        print(f\"Found test PDF file: {test_pdf_path}\")\n",
    "        result_pdf = classify_ecg(test_pdf_path, voting_loaded, is_pdf=True, debug=True)\n",
    "        print(f\"PDF file classification result (segment voting): {result_pdf}\")\n",
    "    else:\n",
    "        print(f\"Test PDF file not found: {test_pdf_path}\")\n",
    "        print(\"Skipping PDF file classification test.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error testing PDF file: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
